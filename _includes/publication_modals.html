<!-- Publication Modals (Auto-generated from _data/publications.yml) -->
<!-- Used by both home page and publications page -->

{% for pub in site.data.publications.publications %}
<!-- [{{ pub.id | upcase }}] {{ pub.venue_short }} {{ pub.year }} Modal -->
<div class="publication-modal modal-{{ pub.year }}" id="{{ pub.id }}">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('{{ pub.id }}')">&times;</button>
      <h2 class="text-uppercase">{{ pub.title }}</h2>
      <div>
        <p class="item-intro text-muted">{{ pub.venue }}</p>
        {% if pub.links %}
        <div class="modal-header-links">
          {% if pub.links.pdf %}
          <a href="{{ pub.links.pdf }}" target="_blank" class="modal-icon-link active pdf-link" title="PDF" onclick="event.stopPropagation()">
            <i class="fas fa-file-pdf"></i>
          </a>
          {% endif %}
          {% if pub.links.website %}
          <a href="{{ pub.links.website }}" target="_blank" class="modal-icon-link active website-link" title="Website" onclick="event.stopPropagation()">
            <i class="fas fa-globe"></i>
          </a>
          {% endif %}
          {% if pub.links.github %}
          <a href="{{ pub.links.github }}" target="_blank" class="modal-icon-link active github-link" title="GitHub" onclick="event.stopPropagation()">
            <i class="fab fa-github"></i>
          </a>
          {% endif %}
          {% if pub.links.video %}
          <a href="{{ pub.links.video }}" target="_blank" class="modal-icon-link active video-link" title="Video" onclick="event.stopPropagation()">
            <i class="fas fa-video"></i>
          </a>
          {% endif %}
        </div>
        {% endif %}
      </div>
    </div>
    <div class="publication-modal-body">
      <!-- Method Image Section (Only show if image exists) -->
      {% if pub.image %}
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Architecture</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/{{ pub.image }}" alt="{{ pub.venue_short }} {{ pub.year }} Method" class="method-image">
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c24' %}
      <!-- IPRF Video Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Results</h5>
        <div class="iprf-results-container">
          <!-- Room Scene with Starry Night style -->
          <div class="iprf-result-row">
            <div class="iprf-style-image-col">
              <h6 class="style-label">Style Image</h6>
              <img src="{{ site.baseurl }}/assets/img/publications/iprf/styles/starry_night.jpg" alt="Starry Night Style" class="iprf-style-image">
            </div>
            <div class="iprf-video-col">
              <h6 class="video-label">Room Scene (Style Transfer Result)</h6>
              <video class="iprf-result-video" autoplay loop muted playsinline>
                <source src="{{ site.baseurl }}/assets/img/publications/iprf/room_122.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <!-- Flower Scene with Tiger style -->
          <div class="iprf-result-row">
            <div class="iprf-style-image-col">
              <h6 class="style-label">Style Image</h6>
              <img src="{{ site.baseurl }}/assets/img/publications/iprf/styles/tiger_neon.jpg" alt="Tiger Neon Style" class="iprf-style-image">
            </div>
            <div class="iprf-video-col">
              <h6 class="video-label">Flower Scene (Style Transfer Result)</h6>
              <video class="iprf-result-video" autoplay loop muted playsinline>
                <source src="{{ site.baseurl }}/assets/img/publications/iprf/flower_14.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <!-- Horns Scene with Totoro style -->
          <div class="iprf-result-row">
            <div class="iprf-style-image-col">
              <h6 class="style-label">Style Image</h6>
              <img src="{{ site.baseurl }}/assets/img/publications/iprf/styles/totoro_catbus.jpg" alt="Totoro Catbus Style" class="iprf-style-image">
            </div>
            <div class="iprf-video-col">
              <h6 class="video-label">Horns Scene (Style Transfer Result)</h6>
              <video class="iprf-result-video" autoplay loop muted playsinline>
                <source src="{{ site.baseurl }}/assets/img/publications/iprf/horns_139.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>

      <!-- IPRF Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Photorealistic style transfer in neural radiance fields (NeRF) aims to modify the color characteristics of a 3D scene without altering its underlying geometry. Although recent approaches have achieved promising results, they often suffer from limited style diversity, focusing primarily on global color shifts. In contrast, artistic style transfer methods offer richer stylization but usually distort scene geometry, thereby reducing realism.</p>

          <p>In this work, we present Intrinsic-guided Photorealistic Style Transfer (IPRF), a novel framework that leverages intrinsic image decomposition to decouple a scene into albedo and shading components. By introducing tailored loss functions in both domains, IPRF aligns the texture and color of the content scene to those of a style image while faithfully preserving geometric structure and lighting.</p>

          <p>Furthermore, we propose Tuning-assisted Style Interpolation (TSI), a real-time technique for exploring the trade-off between photorealism and artistic expression through a weighted combination of albedo-oriented and shading-oriented radiance fields. Experimental results demonstrate that IPRF achieves a superior balance between naturalism and artistic expression compared to state-of-the-art methods, offering a versatile solution for 3D content creation in various fields, including digital art, virtual reality, and game design.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'sidl' %}
      <!-- SIDL Demo Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Interactive Demo</h5>
        <div class="sidl-demo-container">
          <div id="contaminantTypes" class="btn-group mb-3" role="group">
            <label class="btn btn-outline-primary active">
              <input type="radio" name="contaminant" value="dust" checked> Dust
            </label>
            <label class="btn btn-outline-primary">
              <input type="radio" name="contaminant" value="finger"> Fingerprint
            </label>
            <label class="btn btn-outline-primary">
              <input type="radio" name="contaminant" value="scratch"> Scratch
            </label>
            <label class="btn btn-outline-primary">
              <input type="radio" name="contaminant" value="water"> Water
            </label>
          </div>
          <div class="sidl-image-wrapper">
            <div id="sidlInputContainer" class="sidl-input-container">
              <img id="sidlInputImage" src="" alt="Input Image">
            </div>
            <img id="sidlTargetImage" src="" alt="Target Image" class="sidl-target-image">
            <div id="sidlDivider" class="sidl-divider">
              <div id="sidlHandle" class="sidl-handle">
                <i class="fas fa-grip-lines-vertical"></i>
              </div>
            </div>
          </div>
          <p class="text-center mt-2 text-muted"><small>Drag the slider to compare input (contaminated) and output (restored) images</small></p>
        </div>
      </div>

      <!-- SIDL Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Smartphone cameras are ubiquitous in daily life, yet their performance can be severely impacted by dirty lenses, leading to degraded image quality. This issue is often overlooked in image restoration research, which assumes ideal or controlled lens conditions. To address this gap, we introduced SIDL (Smartphone Images with Dirty Lenses), a novel dataset designed to restore images captured through contaminated smartphone lenses. SIDL contains diverse real-world images taken under various lighting conditions and environments. These images feature a wide range of lens contaminants, including water drops, fingerprints, and dust. Each contaminated image is paired with a clean reference image, enabling supervised learning approaches for restoration tasks.</p>

          <p>To evaluate the challenge posed by SIDL, various state-of-the-art restoration models were trained and compared on this dataset. Their performances achieved some level of restoration but did not adequately address the diverse and realistic nature of the lens contaminants in SIDL. This challenge highlights the need for more robust and adaptable image restoration techniques for restoring images with dirty lenses.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c25' %}
      <!-- DynScene Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Robotic manipulation in embodied AI critically depends on large-scale, high-quality datasets that reflect realistic object interactions and physical dynamics. However, existing data collection pipelines are often slow, expensive, and heavily reliant on manual efforts. We present DynScene, a diffusion-based framework for generating dynamic robotic manipulation scenes directly from textual instructions. Unlike prior methods that focus solely on static environments or isolated robot actions, DynScene decomposes the generation into two phases static scene synthesis and action trajectory generation allowing fine-grained control and diversity. Our model enhances realism and physical feasibility through scene refinement (layout sampling, quaternion quantization) and leverages residual action representation to enable action augmentation, generating multiple diverse trajectories from a single static configuration.</p>

          <p>Experiments show DynScene achieves 26.8× faster generation, 1.84× higher accuracy, and 28% greater action diversity than human-crafted data. Furthermore, agents trained with DynScene exhibit up to 19.4% higher success rates across complex manipulation tasks. Our approach paves the way for scalable, automated dataset generation in robot learning.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j12' %}
      <!-- DeepGAM Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Deep neural networks have achieved significant performance breakthroughs across a range of tasks. For diagnosing depression, there has been increasing attention on estimating depression status from personal medical data. However, the neural networks often act as black boxes, making it difficult to discern the individual effects of each input component. To alleviate this problem, we proposed a deep-learning-based generalized additive model called DeepGAM to improve the interpretability of depression diagnosis. We utilized the baseline cross-sectional data from the Heart and Soul Study to achieve our study's aim. DeepGAM incorporates additive functions based on a neural network that learns to discern the positive and negative impacts of the values of individual components. The network architecture and the objective function are designed to constrain and regularize the output values for interpretability.</p>

          <p>Moreover, we used a direct-through estimator (STE) to select important features using gradient descent. The STE enables machine learning models to maintain their performance using a few features and interpretable function visualizations. DeepGAM achieved the highest AUC (0.600) and F1-score (0.387), outperforming neural networks and IGANN. The five features selected via STE performed comparably to 99 features and surpassed traditional methods such as Lasso and Boruta. Additionally, analyses highlighted DeepGAM's interpretability and performance on public datasets. In conclusion, DeepGAM with STE demonstrated accurate and interpretable performance in predicting depression compared to existing machine learning methods.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j11' %}
      <!-- Dog Cough Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>We propose a diagnostic system for identifying bronchial diseases by analyzing dog cough sounds. We collected a dataset consisting of 124 healthy dog cough sounds obtained from open sources and 94 dog cough sounds with bronchial diseases obtained from YouTube, and performed a total of 218 recordings. These cough sounds were segmented into 423 separate cough datasets to improve the details and accuracy of their analysis. Additionally, data augmentation techniques such as noise addition, pitch shifting, time stretching, and volume scaling were applied, increasing the dataset size by 7 times. This resulted in 1,526 training and testing samples for multiple coughs and 2,961 samples for single coughs.</p>

          <p>The disease prediction system leverages three different neural network models, multilayer perceptron (MLP), convolutional neural network (CNN), and recurrent neural network (RNN), to evaluate their effectiveness in detecting bronchial diseases. In our experiments, we found that the single cough dataset outperformed the multiple cough dataset, with the CNN achieving the highest accuracy, precision, AUC, and F1 scores compared to the RNN and MLP. The study highlights the potential of machine learning in improving diagnostic accuracy for veterinary medicine, suggesting that integrating different models could enhance diagnostic tools, thereby contributing to better health outcomes for dogs.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j10' %}
      <!-- Baseball Player Pose Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Human pose estimation (HPE) is challenging due to the need to accurately capture rapid and occluded body movements, often resulting in uncertain predictions. In the context of fast sports actions like baseball swings, existing HPE methods insufficiently leverage domain-specific prior knowledge about these movements. To address this gap, we propose the Baseball Player Pose Corrector (BPPC), an optimization framework that utilizes high-quality 3D standard motion data to refine 2D keypoints in baseball swing videos. BPPC operates in two stages: first, it aligns the 3D standard motion to test swing videos through action recognition, offset learning, and 3D-to-2D projection. Next, it applies movement-aware optimization to refine the keypoints, ensuring robustness to variations in swing patterns.</p>

          <p>Notably, BPPC does not rely on additional datasets; it only requires manually annotated 3D standard motion data for baseball swings. Experimental results demonstrate that BPPC improves keypoint estimation accuracy by up to 2.4% on a baseball swing dataset, particularly enhancing keypoints with confidence scores below 0.5. Qualitative analysis further highlights BPPC's ability to correct rapidly moving joints, such as elbows and wrists.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c20j9' %}
      <!-- tDCS KOA Pain Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Transcranial direct current stimulation (tDCS) has been recognized as a safe and effective intervention for treating knee osteoarthritis (KOA) pain; however, research has suggested the heterogeneity of treatment effects across participants. This study aimed to identify the sociodemographic and clinical predictors of such heterogeneity in older adults with symptomatic KOA undergoing tDCS, thereby enhancing personalized treatment strategies. Specifically, we analyzed active and sham tDCS groups separately to account for placebo or sham effects. This study entailed secondary data analysis of a double-blind, randomized, sham-controlled, phase II, parallel-group pilot clinical trial involving 120 participants with KOA pain. These participants were assigned to 15 daily telehealth-delivered sessions of either active 2-mA tDCS (n=60) for 20 min or sham stimulation (n=60) over 3 weeks. The primary outcome was the change in Western Ontario and McMaster Universities Osteoarthritis Index (WOMAC) pain subscale scores, measured from baseline to after the 15 tDCS sessions in both the active and sham groups.</p>

          <p>Predictive modeling using random forest (RF) and artificial neural network (ANN) algorithms was utilized, with model performance assessed based on R-squared values. The impact of predictive features on treatment outcomes was examined using several feature selection methods, including Lasso, BorutaSHAP, Chi2, F-regression, and R-regression. The RF and ANN models both effectively predicted treatment effects, indicating the potential of machine learning to enhance patient-specific treatment strategies. In the active group, the predominant features included age, average heat pain tolerance at the knee at baseline, baseline WOMAC functional score, and the duration of KOA. In the sham group, the major features comprised the duration of KOA, Kellgren–Lawrence scale score of the affected knee, baseline pain catastrophizing score, average heat pain tolerance at the knee at baseline, and baseline WOMAC functional score. Characterizing these predictive factors can inform personalized tDCS protocols, potentially improving treatment effects.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c17j8' %}
      <!-- Chronic Pain Trajectories Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Chronic pain is a major public health problem affecting approximately 100 million Americans and United States military Veterans, who constitute a particularly vulnerable group. While pain research in Veterans is actively underway, information on the longitudinal course of pain in this population is limited. This study aimed to 1) identify the various longitudinal pain status trajectories among older Veterans over a 10-year period and 2) detect factors predicting membership in the worsening trajectory of chronic pain. We analyzed data from 619 Veterans (mean age: 58.5 years) participating in the Mind Your Heart Study, an ongoing prospective cohort study examining diverse health outcomes among Veterans. Initially, we employed a generalized mixture model to identify pain trajectory classes using Brief Pain Inventory (BPI) pain intensity subscale score collected at 2-, 5-, and 10-year intervals. Two distinct trajectories were identified—low and high—both of which remained relatively stable.</p>

          <p>Subsequently, several feature selection methods extracted the predominant features from participants' baseline characteristics that predicted membership in the high vs. low pain trajectory. These included: prior arthritis diagnosis; prior post-traumatic stress disorder (PTSD) diagnosis; depression symptoms; PTSD symptoms of avoidance, hyperarousal, and negative mood alterations; physical functioning; sleep quality; and overall health. The scikit-learn RandomForestClassifier, utilizing the refined feature set, achieved a classification accuracy of 0.79, yielding results nearly identical to those obtained using all 261 features. These findings are clinically informative and pertinent, highlighting potential intervention targets warranting intensive pain care plans based on probable long-term prognosis and discussing early treatment strategies among older Veterans.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j4' %}
      <!-- Learning Controllable ISP Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Editing flat-looking images into stunning photographs requires skill and time. Automated image enhancement algorithms have attracted increased interest by generating high-quality images without user interaction. However, the quality assessment of a photograph is subjective. Even in tone and color adjustments, a single photograph of auto-enhancement is challenging to fit user preferences which are subtle and even changeable. To address this problem, we present a semi-automatic image enhancement algorithm that can generate high-quality images with multiple styles by controlling a few parameters.</p>

          <p>We first disentangle photo retouching skills from high-quality images and build an efficient enhancement system for each skill. Specifically, an encoder-decoder framework encodes the retouching skills into latent codes and decodes them into the parameters of image signal processing (ISP) functions. The ISP functions are computationally efficient and consist of only 19 parameters. Despite our approach requiring multiple inferences to obtain the desired result, experimental results present that the proposed method achieves state-of-the-art performances on the benchmark dataset for image quality and model efficiency.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c15_2023' %}
      <!-- NERDS Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>We aim to train accurate denoising networks for smartphone/digital cameras from single noisy images. Downscaling is commonly used as a practical denoiser for low-resolution images. Based on this processing, we found that the pixel variance of the natural images is more robust to downscaling than the pixel variance of the camera noises. Intuitively, downscaling easily removes high-frequency noises than natural textures. To utilize this property, we can adopt noisy/clean image synthesis at low-resolution to train camera denoisers. On this basis, we propose a new solution pipeline -- NERDS that estimates camera noises and synthesizes noisy-clean image pairs from only noisy images. In particular, it first models the noise in raw-sensor images as a Poisson-Gaussian distribution, then estimates the noise parameters using the difference of pixel variances by downscaling. We formulate the noise estimation as a gradient-descent-based optimization problem through a reparametrization trick.</p>

          <p>We further introduce a new Image Signal Processor (ISP) estimation method that enables denoiser training in a human-readable RGB space by transforming the synthetic raw images to the style of a given RGB noisy image. The noise and ISP estimations utilize rich augmentation to synthesize image pairs for denoiser training. Experiments show that our NERDS can accurately train CNN-based denoisers (e.g., DnCNN, ResNet-style network) outperforming previous noise-synthesis-based and self-supervision-based denoisers in real datasets.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j3' %}
      <!-- Learning to Learn Task-Adaptive Hyperparameters Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>In few-shot learning scenarios, the challenge is to generalize and perform well on new unseen examples when only very few labeled examples are available for each task. Model-agnostic meta-learning (MAML) has gained the popularity as one of the representative few-shot learning methods for its flexibility and applicability to diverse problems. However, MAML and its variants often resort to a simple loss function without any auxiliary loss function or regularization terms that can help achieve better generalization. The problem lies in that each application and task may require different auxiliary loss function, especially when tasks are diverse and distinct.</p>

          <p>Instead of attempting to hand-design an auxiliary loss function for each application and task, we introduce a new meta-learning framework with a loss function that adapts to each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss Function (MeTAL), demonstrates the effectiveness and the flexibility across various domains, such as few-shot classification and few-shot regression.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j2' %}
      <!-- Fine-Grained NAS Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>The goal of filter pruning is to search for unimportant filters to remove in order to make convolutional neural networks (CNNs) efficient without sacrificing the performance in the process. The challenge lies in finding information that can help determine how important or relevant each filter is with respect to the final output of neural networks. In this work, we share our observation that the batch normalization (BN) parameters of pre-trained CNNs can be used to estimate the feature distribution of activation outputs, without processing of training data.</p>

          <p>Upon observation, we propose a simple yet effective filter pruning method by evaluating the importance of each filter based on the BN parameters of pre-trained CNNs. The experimental results on CIFAR-10 and ImageNet demonstrate that the proposed method can achieve outstanding performance with and without fine-tuning in terms of the trade-off between the accuracy drop and the reduction in computational complexity and number of parameters of pruned networks.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c14' %}
      <!-- CADyQ Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance.</p>

          <p>To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without the significant sacrifice in SR performance, compared to other quantization methods.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c13' %}
      <!-- Attentive Fine-Grained Structured Sparsity Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Video frame interpolation aims to synthesize accurate intermediate frames given a low-frame-rate video. While the quality of the generated frames is increasingly getting better, state-of-the-art models have become more and more computationally expensive. However, local regions with small or no motion can be easily interpolated with simple models and do not require such heavy compute, whereas some regions may not be correct even after inference through a large model.</p>

          <p>Thus, we propose an effective framework that assigns varying amounts of computation for different regions. Our dynamic architecture first calculates the approximate motion magnitude to use as a proxy for the difficulty levels for each region, and decides the depth of the model and the scale of the input. Experimental results show that static regions pass through a smaller number of layers, while the regions with larger motion are downscaled for better motion reasoning. In doing so, we demonstrate that the proposed framework can significantly reduce the computation cost (FLOPs) while maintaining the performance, often up to 50% when interpolating a 2K resolution video.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c12' %}
      <!-- DAQ Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance.</p>

          <p>To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without the significant sacrifice in SR performance, compared to other quantization methods.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c11' %}
      <!-- Batch Normalization Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>The goal of filter pruning is to search for unimportant filters to remove in order to make convolutional neural networks (CNNs) efficient without sacrificing the performance in the process. The challenge lies in finding information that can help determine how important or relevant each filter is with respect to the final output of neural networks. In this work, we share our observation that the batch normalization (BN) parameters of pre-trained CNNs can be used to estimate the feature distribution of activation outputs, without processing of training data.</p>

          <p>Upon observation, we propose a simple yet effective filter pruning method by evaluating the importance of each filter based on the BN parameters of pre-trained CNNs. The experimental results on CIFAR-10 and ImageNet demonstrate that the proposed method can achieve outstanding performance with and without fine-tuning in terms of the trade-off between the accuracy drop and the reduction in computational complexity and number of parameters of pruned networks.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c10' %}
      <!-- Meta-Learning with Task-Adaptive Loss Function Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>In few-shot learning scenarios, the challenge is to generalize and perform well on new unseen examples when only very few labeled examples are available for each task. Model-agnostic meta-learning (MAML) has gained the popularity as one of the representative few-shot learning methods for its flexibility and applicability to diverse problems. However, MAML and its variants often resort to a simple loss function without any auxiliary loss function or regularization terms that can help achieve better generalization. The problem lies in that each application and task may require different auxiliary loss function, especially when tasks are diverse and distinct.</p>

          <p>Instead of attempting to hand-design an auxiliary loss function for each application and task, we introduce a new meta-learning framework with a loss function that adapts to each task. Our proposed framework, named Meta-Learning with Task-Adaptive Loss Function (MeTAL), demonstrates the effectiveness and the flexibility across various domains, such as few-shot classification and few-shot regression.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c9' %}
      <!-- Motion-Aware Dynamic Architecture Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Video frame interpolation aims to synthesize accurate intermediate frames given a low-frame-rate video. While the quality of the generated frames is increasingly getting better, state-of-the-art models have become more and more computationally expensive. However, local regions with small or no motion can be easily interpolated with simple models and do not require such heavy compute, whereas some regions may not be correct even after inference through a large model.</p>

          <p>Thus, we propose an effective framework that assigns varying amounts of computation for different regions. Our dynamic architecture first calculates the approximate motion magnitude to use as a proxy for the difficulty levels for each region, and decides the depth of the model and the scale of the input. Experimental results show that static regions pass through a smaller number of layers, while the regions with larger motion are downscaled for better motion reasoning. In doing so, we demonstrate that the proposed framework can significantly reduce the computation cost (FLOPs) while maintaining the performance, often up to 50% when interpolating a 2K resolution video.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c8' %}
      <!-- Searching for Controllable Image Restoration Networks Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>We present a novel framework for controllable image restoration that can effectively restore multiple types and levels of degradation of a corrupted image. The proposed model, named TASNet, is automatically determined by our neural architecture search algorithm, which optimizes the efficiency-accuracy trade-off of the candidate model architectures. Specifically, we allow TASNet to share the early layers across different restoration tasks and adaptively adjust the remaining layers with respect to each task.</p>

          <p>The shared task-agnostic layers greatly improve the efficiency while the task-specific layers are optimized for restoration quality, and our search algorithm seeks for the best balance between the two. We also propose a new data sampling strategy to further improve the overall restoration performance. As a result, TASNet achieves significantly faster GPU latency and lower FLOPs compared to the existing state-of-the-art models, while also showing visually more pleasing outputs.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c7' %}
      <!-- Real-Time Video Super-Resolution Mobile AI 2021 Challenge Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Video super-resolution has recently become one of the most important mobile-related problems due to the rise of video communication and streaming services. While many solutions have been proposed for this task, the majority of them are too computationally expensive to run on portable devices with limited hardware resources. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop an end-to-end deep learning-based video super-resolution solutions that can achieve a real-time performance on mobile GPUs.</p>

          <p>The participants were provided with the REDS dataset and trained their models to do an efficient 4X video upscaling. The runtime of all models was evaluated on the OPPO Find X2 smartphone with the Snapdragon 865 SoC capable of accelerating floating-point networks on its Adreno GPU. The proposed solutions are fully compatible with any mobile GPU and can upscale videos to HD resolution at up to 80 FPS while demonstrating high fidelity results. A detailed description of all models developed in the challenge is provided in this paper.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c5' %}
      <!-- Channel Attention Is All You Need Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Prevailing video frame interpolation techniques rely heavily on optical flow estimation and require additional model complexity and computational cost; it is also susceptible to error propagation in challenging scenarios with large motion and heavy occlusion. To alleviate the limitation, we propose a simple but effective deep neural network for video frame interpolation, which is end-to-end trainable and is free from a motion estimation network component.</p>

          <p>Our algorithm employs a special feature reshaping operation, referred to as PixelShuffle, with a channel attention, which replaces the optical flow computation module. The main idea behind the design is to distribute the information in a feature map into multiple channels and extract motion information by attending the channels for pixel-level frame synthesis. The model given by this principle turns out to be effective in the presence of challenging motion and occlusion. We construct a comprehensive evaluation benchmark and demonstrate that the proposed approach achieves outstanding performance compared to the existing models with a component for optical flow computation.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c3' %}
      <!-- Task-Aware Image Downscaling Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Image downscaling is one of the most classical problems in computer vision that aims to preserve the visual appearance of the original image when it is resized to a smaller scale. Upscaling a small image back to its original size is a difficult and ill-posed problem due to information loss that arises in the downscaling process. In this paper, we present a novel technique called task-aware image downscaling to support an upscaling task.</p>

          <p>We propose an auto-encoder-based framework that enables joint learning of the downscaling network and the upscaling network to maximize the restoration performance. Our framework is efficient, and it can be generalized to handle an arbitrary image resizing operation. Experimental results show that our task-aware downscaled images greatly improve the performance of the existing state-of-the-art super-resolution methods. In addition, realistic images can be recovered by recursively applying our scaling model up to an extreme scaling factor of x128. We also validate our model's generalization capability by applying it to the task of image colorization.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c2' %}
      <!-- NTIRE 2017 Challenge Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>This paper reviews the first challenge on single image super-resolution (restoration of rich details in an low resolution image) with focus on proposed solutions and results. A new DIVerse 2K resolution image dataset (DIV2K) was employed. The challenge had 6 competitions divided into 2 tracks with 3 magnification factors each. Track 1 employed the standard bicubic downscaling setup, while Track 2 had unknown downscaling operators (blur kernel and decimation) but learnable through low and high res train images. Each competition had ∼ 100 registered participants and 20 teams competed in the final testing phase. They gauge the state-of-the-art in single image super-resolution.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c1' %}
      <!-- Enhanced Deep Residual Networks Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Recent research on super-resolution has progressed with the development of deep convolutional neural networks (DCNN). In particular, residual learning techniques exhibit improved performance. In this paper, we develop an enhanced deep super-resolution network (EDSR) with performance exceeding those of current state-of-the-art SR methods. The significant performance improvement of our model is due to optimization by removing unnecessary modules in conventional residual networks.</p>

          <p>The performance is further improved by expanding the model size while we stabilize the training procedure. We also propose a new multi-scale deep super-resolution system (MDSR) and training method, which can reconstruct high-resolution images of different upscaling factors in a single model. The proposed methods show superior performance over the state-of-the-art methods on benchmark datasets and prove its excellence by winning the NTIRE2017 Super-Resolution Challenge.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j7' %}
      <!-- Machine Learning-Based Etiologic Subtyping of Ischemic Stroke Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Ischemic stroke is a major cause of mortality worldwide. Proper etiological subtyping of ischemic stroke is crucial for tailoring treatment strategies. This study explored the utility of circulating microRNAs encapsulated in extracellular vesicles (EV-miRNAs) to distinguish the following ischemic stroke subtypes: large artery atherosclerosis (LAA), cardioembolic stroke (CES), and small artery occlusion (SAO). Using next-generation sequencing (NGS) and machine-learning techniques, we identified differentially expressed miRNAs (DEMs) associated with each subtype. Through patient selection and diagnostic evaluation, a cohort of 70 patients with acute ischemic stroke was classified: 24 in the LAA group, 24 in the SAO group, and 22 in the CES group.</p>

          <p>Our findings revealed distinct EV-miRNA profiles among the groups, suggesting their potential as diagnostic markers. Machine-learning models, particularly logistic regression models, exhibited a high diagnostic accuracy of 92% for subtype discrimination. The collective influence of multiple miRNAs was more crucial than that of individual miRNAs. Additionally, bioinformatics analyses have elucidated the functional implications of DEMs in stroke pathophysiology, offering insights into the underlying mechanisms. Despite limitations like sample size constraints and retrospective design, our study underscores the promise of EV-miRNAs coupled with machine learning for ischemic stroke subtype classification. Further investigations are warranted to validate the clinical utility of the identified EV-miRNA biomarkers in stroke patients.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'j1' %}
      <!-- Machine learning-based predictive modeling of depression Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>We aimed to develop prediction models for depression among U.S. adults with hypertension using various machine learning (ML) approaches. Moreover, we analyzed the mechanisms of the developed models. This cross-sectional study included 8,628 adults with hypertension (11.3% with depression) from the National Health and Nutrition Examination Survey (2011–2020). We selected several significant features using feature selection methods to build the models. Data imbalance was managed with random down-sampling. Six different ML classification methods implemented in the R package caret—artificial neural network, random forest, AdaBoost, stochastic gradient boosting, XGBoost, and support vector machine—were employed with 10-fold cross-validation for predictions. Model performance was assessed by examining the area under the receiver operating characteristic curve (AUC), accuracy, precision, sensitivity, specificity, and F1-score. For an interpretable algorithm, we used the variable importance evaluation function in caret.</p>

          <p>Of all classification models, artificial neural network trained with selected features (n = 30) achieved the highest AUC (0.813) and specificity (0.780) in predicting depression. Support vector machine predicted depression with the highest accuracy (0.771), precision (0.969), sensitivity (0.774), and F1-score (0.860). The most frequent and important features contributing to the models included the ratio of family income to poverty, triglyceride level, white blood cell count, age, sleep disorder status, the presence of arthritis, hemoglobin level, marital status, and education level. In conclusion, ML algorithms performed comparably in predicting depression among hypertensive populations. Furthermore, the developed models shed light on variables' relative importance, paving the way for further clinical research.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c6' %}
      <!-- Meta-Learning with Adaptive Hyperparameters Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>The ability to quickly learn and generalize from only few examples is an essential goal of few-shot learning. Gradient-based meta-learning algorithms effectively tackle the problem by learning how to learn novel tasks. In particular, model-agnostic meta-learning (MAML) encodes the prior knowledge into a trainable initialization, which allowed for fast adaptation to few examples. Despite its popularity, several recent works question the effectiveness of MAML initialization especially when test tasks are different from training tasks, thus suggesting various methodologies to improve the initialization.</p>

          <p>Instead of searching for a better initialization, we focus on a complementary factor in MAML framework, the inner-loop optimization (or fast adaptation). Consequently, we propose a new weight update rule that greatly enhances the fast adaptation process. Specifically, we introduce a small meta-network that can adaptively generate per-step hyperparameters: learning rate and weight decay coefficients. The experimental results validate that the Adaptive Learning of hyperparameters for Fast Adaptation (ALFA) is the equally important ingredient that was often neglected in the recent few-shot learning approaches. Surprisingly, fast adaptation from random initialization with ALFA can already outperform MAML.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c4' %}
      <!-- AIM 2019 Challenge Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>Videos contain various types and strengths of motions that may look unnaturally discontinuous in time when the recorded frame rate is low. This paper reviews the first AIM challenge on video temporal super-resolution (frame interpolation) with a focus on the proposed solutions and results. From low-frame-rate (15 fps) video sequences, the challenge participants are asked to submit higher-frame-rate (60 fps) video sequences by estimating temporally intermediate frames. We employ the REDS VTSR dataset derived from diverse videos captured in a hand-held camera for training and evaluation purposes.</p>

          <p>The competition had 62 registered participants, and a total of 8 teams competed in the final testing phase. The challenge winning methods achieve the state-of-the-art in video temporal super-resolution.</p>
        </div>
      </div>
      {% endif %}

      {% if pub.id == 'c23' %}
      <!-- UDT Abstract Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Abstract</h5>
        <div class="abstract-content">
          <p>{{ pub.abstract }}</p>
        </div>
      </div>
      {% endif %}

      <div class="citation-box">
        <p>{{ pub.citation }}</p>
      </div>
      {% if pub.notes %}
      <p class="mt-3"><small>{{ pub.notes }}</small></p>
      {% endif %}
    </div>
  </div>
</div>
{% endfor %}

<script>
function openPublicationModal(modalId) {
  const modal = document.getElementById(modalId);
  if (modal) {
    modal.classList.add("active");
    document.body.style.overflow = "hidden";
  }
}

function closePublicationModal(modalId) {
  const modal = document.getElementById(modalId);
  if (modal) {
    modal.classList.remove("active");
    document.body.style.overflow = "";
  }
}

// Close modal when clicking outside content
document.addEventListener("click", function(event) {
  if (event.target.classList.contains("publication-modal")) {
    const modalId = event.target.id;
    closePublicationModal(modalId);
  }
});

// Close modal with Escape key
document.addEventListener("keydown", function(event) {
  if (event.key === "Escape") {
    const activeModal = document.querySelector(".publication-modal.active");
    if (activeModal) {
      closePublicationModal(activeModal.id);
    }
  }
});

// Open modal from URL hash (e.g., #sidl-open)
document.addEventListener("DOMContentLoaded", function() {
  const hash = window.location.hash;
  if (hash && hash.endsWith("-open")) {
    const modalId = hash.substring(1, hash.length - 5); // Remove # and -open
    setTimeout(function() {
      openPublicationModal(modalId);
    }, 100);
  }
});
</script>

<style>
/* Publication Modal Styles */
.publication-modal {
  display: none;
  position: fixed;
  z-index: 9999;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgba(0, 0, 0, 0.8) !important;
  opacity: 0;
  transition: opacity 0.3s ease;
  align-items: center;
  justify-content: center;
  padding: 2rem;
}

.publication-modal.active {
  display: flex;
  opacity: 1 !important;
}

.publication-modal-content {
  background: white !important;
  opacity: 1 !important;
  border-radius: 15px;
  box-shadow: 0 25px 50px rgba(0, 0, 0, 0.3);
  max-height: 90vh;
  overflow: hidden;
  position: relative;
  margin: auto;
  width: 85vw;
  max-width: 85vw;
  animation: slideIn 0.3s ease-out;
  display: flex;
  flex-direction: column;
}

@media (min-width: 992px) {
  .publication-modal-content {
    width: 70vw;
    max-width: 70vw;
  }
}

@media (min-width: 1200px) {
  .publication-modal-content {
    width: 65vw;
    max-width: 65vw;
  }
}

@keyframes slideIn {
  from {
    transform: translateY(-50px);
    opacity: 0;
  }
  to {
    transform: translateY(0);
    opacity: 1;
  }
}

.publication-modal-header {
  padding: 30px;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border-radius: 12px 12px 0 0;
  position: relative;
}

/* Year-specific modal header colors */
.modal-2025 .publication-modal-header {
  background: linear-gradient(135deg, #7a8dd6, #8769a8);
}

.modal-2024 .publication-modal-header {
  background: linear-gradient(135deg, #f39c12, #e67e22);
}

.modal-2023 .publication-modal-header {
  background: linear-gradient(135deg, #66b2e4, #4dd5e1);
}

.modal-2022 .publication-modal-header {
  background: linear-gradient(135deg, #5fd68f, #5be1c7);
}

.modal-2021 .publication-modal-header {
  background: linear-gradient(135deg, #e788a1, #eccd6d);
}

.publication-modal-header h2 {
  margin: 0 0 10px 0;
  font-size: 1.8rem;
  font-weight: 700;
}

.publication-modal-header .item-intro {
  margin: 0;
  font-size: 1.1rem;
  color: white;
  -webkit-text-stroke: 0.2px black;
  text-stroke: 0.2px black;
  font-weight: 500;
}

.publication-modal-close {
  position: absolute;
  top: 5px;
  right: 10px;
  color: white;
  font-size: 28px;
  font-weight: bold;
  border: none;
  background: rgba(0, 0, 0, 0.15);
  cursor: pointer;
  transition: all 0.3s ease;
  width: 32px;
  height: 32px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  transform-origin: center center;
  line-height: 1;
  padding: 0;
}

.publication-modal-close:hover {
  background: rgba(0, 0, 0, 0.25);
  transform: rotate(90deg);
}

.publication-modal-body {
  padding: 30px;
  overflow-y: auto;
  flex: 1;
}

.modal-header-links {
  display: flex;
  gap: 12px;
  margin-top: 12px;
}

.modal-icon-link {
  color: rgba(255, 255, 255, 0.8);
  font-size: 1.3rem;
  transition: all 0.3s ease;
  text-decoration: none;
  width: 40px;
  height: 40px;
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 50%;
  background: rgba(255, 255, 255, 0.15);
}

.modal-icon-link:hover {
  color: white;
  background: rgba(255, 255, 255, 0.25);
  transform: translateY(-2px);
}

.modal-icon-link i {
  pointer-events: none;
}

.modal-icon-link.active {
  color: white;
}

.citation-box {
  background: #f8f9fa;
  border-left: 4px solid #667eea;
  padding: 20px;
  border-radius: 8px;
  font-size: 0.95rem;
  line-height: 1.6;
}

.citation-box p {
  margin: 0;
}

.citation-box strong {
  color: #667eea;
}

/* Method Image Section Styling */
.method-image-section {
  margin-bottom: 1.5rem;
}

.method-image-title {
  font-size: 1.1rem;
  font-weight: 600;
  color: #2c3e50;
  margin-bottom: 1rem;
}

.method-image-container {
  width: 85%;
  max-width: 85%;
  margin: 0 auto;
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
  background: #f8f9fa;
}

.method-image {
  width: 100%;
  height: auto;
  display: block;
  object-fit: contain;
}

.abstract-content {
  padding: 20px;
  background: #f8f9fa;
  border-radius: 10px;
  line-height: 1.8;
}

.abstract-content p {
  margin-bottom: 1rem;
  text-align: justify;
  color: #2c3e50;
}

.abstract-content p:last-child {
  margin-bottom: 0;
}

@media (max-width: 768px) {
  .publication-modal-content {
    width: 95%;
    margin: 10% auto;
  }

  .publication-modal-header {
    padding: 20px;
  }

  .publication-modal-header h2 {
    font-size: 1.4rem;
  }

  .publication-modal-header .item-intro {
    font-size: 0.95rem;
  }

  .publication-modal-body {
    padding: 20px;
  }

  .citation-box {
    font-size: 0.85rem;
    padding: 15px;
  }
}

/* IPRF Results Container */
.iprf-results-container {
  display: flex;
  flex-direction: column;
  gap: 30px;
  padding: 20px;
  background: #f8f9fa;
  border-radius: 10px;
}

.iprf-result-row {
  display: grid;
  grid-template-columns: 1fr 2fr;
  gap: 20px;
  align-items: center;
  background: white;
  padding: 15px;
  border-radius: 10px;
  box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
}

.iprf-style-image-col,
.iprf-video-col {
  display: flex;
  flex-direction: column;
  align-items: center;
}

.style-label,
.video-label {
  font-size: 0.9rem;
  font-weight: 600;
  color: #2c3e50;
  margin-bottom: 10px;
  text-align: center;
  width: 100%;
}

.iprf-style-image {
  max-width: 100%;
  max-height: 350px;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  background: white;
  object-fit: contain;
}

/* Individual size adjustments for each style image */
.iprf-results-container .iprf-result-row:nth-child(1) .iprf-style-image {
  max-height: 196px; /* Starry Night (room_122) */
}

.iprf-results-container .iprf-result-row:nth-child(2) .iprf-style-image {
  max-height: 235px; /* Tiger (flower_14) - 20% increase */
}

.iprf-results-container .iprf-result-row:nth-child(3) .iprf-style-image {
  max-height: 256px; /* Totoro (horns_139) - 10% decrease */
}

.iprf-result-video {
  width: 100%;
  max-height: 350px;
  border-radius: 8px;
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
  background: white;
}

/* SIDL Demo Styles */
.sidl-demo-container {
  padding: 20px;
  background: #f8f9fa;
  border-radius: 10px;
}

.sidl-image-wrapper {
  position: relative;
  width: 100%;
  max-width: 600px;
  margin: 0 auto;
  aspect-ratio: 4/3;
  overflow: hidden;
  border-radius: 8px;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
  cursor: col-resize;
}

.sidl-input-container {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  clip-path: inset(0 50% 0 0);
  z-index: 2;
}

.sidl-input-container img {
  width: 100%;
  height: 100%;
  object-fit: cover;
  display: block;
}

.sidl-target-image {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  object-fit: cover;
  display: block;
  z-index: 1;
}

.sidl-divider {
  position: absolute;
  top: 0;
  left: 50%;
  width: 3px;
  height: 100%;
  background: white;
  z-index: 3;
  pointer-events: none;
  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);
}

.sidl-handle {
  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-50%, -50%);
  width: 40px;
  height: 40px;
  background: white;
  border-radius: 50%;
  display: flex;
  align-items: center;
  justify-content: center;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
  cursor: col-resize;
  pointer-events: auto;
  z-index: 4;
}

.sidl-handle i {
  color: #667eea;
  font-size: 1.2rem;
}

#contaminantTypes {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  gap: 10px;
}

#contaminantTypes .btn {
  border: 2px solid #667eea;
  color: #667eea;
  background: white;
  padding: 8px 16px;
  border-radius: 8px;
  font-weight: 500;
  transition: all 0.3s ease;
}

#contaminantTypes .btn:hover {
  background: rgba(102, 126, 234, 0.1);
}

#contaminantTypes .btn.active {
  background: #667eea;
  color: white;
}

#contaminantTypes input[type="radio"] {
  display: none;
}

@media (max-width: 768px) {
  .iprf-results-container {
    gap: 20px;
    padding: 15px;
  }

  .iprf-result-row {
    grid-template-columns: 1fr;
    gap: 15px;
    padding: 12px;
  }

  .sidl-demo-container {
    padding: 15px;
  }

  .sidl-image-wrapper {
    max-width: 100%;
  }
}
</style>

{% include sidl-script.html %}
