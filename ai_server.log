/home/i0179/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
INFO:__main__:Starting Reality Lab Qwen3-4B Server (4-bit) on port 4005...
INFO:__main__:Loading Qwen3-4B tokenizer from local path
INFO:__main__:Loading Qwen3-4B model with 4-bit quantization
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:12<00:25, 12.78s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:19<00:09,  9.12s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  5.03s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:19<00:00,  6.50s/it]
INFO:__main__:âœ… Qwen3-4B model loaded successfully with 4-bit quantization
INFO:__main__:Loading RAG system...
INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cuda:0
INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
INFO:__main__:âœ… RAG system loaded successfully!
INFO:__main__:â±ï¸  Idle timeout enabled: server will shutdown after 120s of inactivity
INFO:__main__:ðŸš€ Qwen3-4B server ready on port 4005!
INFO:__main__:âœ… Running with HTTP (no SSL)
Loading Hierarchical RAG from: /home/i0179/Realitylab-site/ai_server/hierarchical_rag
  Loading embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
  âœ… faculty: 2 docs
  âœ… students: 23 docs
  âœ… alumni: 17 docs
  âœ… news: 19 docs
  âœ… publications: 37 docs
  âœ… lab_info: 1 docs
âœ… Hierarchical RAG ready!
 * Serving Flask app 'qwen3_4b_lowmem'
 * Debug mode: off
INFO:werkzeug:[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:4005
 * Running on http://203.253.25.165:4005
INFO:werkzeug:[33mPress CTRL+C to quit[0m
INFO:__main__:â±ï¸  Server idle for 120.1s (threshold: 120s)
INFO:__main__:ðŸ”Œ Unloading model from GPU...
INFO:__main__:âœ… GPU memory freed. Server remains active for auto-reload.
INFO:werkzeug:127.0.0.1 - - [03/Nov/2025 08:30:49] "[33mGET / HTTP/1.1[0m" 404 -
INFO:werkzeug:127.0.0.1 - - [03/Nov/2025 09:02:33] "[33mGET / HTTP/1.1[0m" 404 -
