---
library_name: transformers
license: cc-by-nc-4.0
language:
- en
- ko
base_model:
- nlpai-lab/KULLM3
- upstage/SOLAR-10.7B-v1.0
---

<a href="https://github.com/nlpai-lab/KULLM">
  <img src="https://huggingface.co/nlpai-lab/KULLM3/resolve/main/kullm_logo.png" width="50%"/>
</a>

#  This repository is the awq quantization version of KULLM3.

The quantization was carried out in a custom branch of [autoawq](https://github.com/casper-hansen/AutoAWQ/). The hyperparameters for quantization are as follows.

```{ "zero_point": True, "q_group_size": 128, "w_bit": 4, "version": "GEMM" }```

It worked using [vllm](https://github.com/vllm-project/vllm). It may not work with other frameworks as they have not been tested.

Below is the README.md for the original model.

---

# KULLM3
Introducing KULLM3, a model with advanced instruction-following and fluent chat abilities.
It has shown remarkable performance in instruction-following, speficially by closely following gpt-3.5-turbo.  
To our knowledge, It is one of the best publicly opened Korean-speaking language models.

For details, visit the [KULLM repository](https://github.com/nlpai-lab/KULLM)

### Model Description

This is the model card of a ğŸ¤— transformers model that has been pushed on the Hub.

- **Developed by:** [NLP&AI Lab](http://nlp.korea.ac.kr/)
- **Language(s) (NLP):** Korean, English
- **License:** CC-BY-NC 4.0
- **Finetuned from model:** [upstage/SOLAR-10.7B-Instruct-v1.0](https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0)

## Example code
### Install Dependencies
```bash
pip install torch transformers==4.38.2 accelerate
```

- In transformers>=4.39.0, generate() does not work well. (as of 2024.4.4.)

### Python code
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

MODEL_DIR = "nlpai-lab/KULLM3"
model = AutoModelForCausalLM.from_pretrained(MODEL_DIR, torch_dtype=torch.float16).to("cuda")
tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

s = "ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ì„œ ì•Œê³  ìˆë‹ˆ?"
conversation = [{'role': 'user', 'content': s}]
inputs = tokenizer.apply_chat_template(
    conversation,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors='pt').to("cuda")
_ = model.generate(inputs, streamer=streamer, max_new_tokens=1024)

# ë„¤, ê³ ë ¤ëŒ€í•™êµì— ëŒ€í•´ ì•Œê³  ìˆìŠµë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” ëŒ€í•œë¯¼êµ­ ì„œìš¸ì— ìœ„ì¹˜í•œ ì‚¬ë¦½ ëŒ€í•™êµë¡œ, 1905ë…„ì— ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëŒ€í•™êµëŠ” í•œêµ­ì—ì„œ ê°€ì¥ ì˜¤ë˜ëœ ëŒ€í•™ ì¤‘ í•˜ë‚˜ë¡œ, ë‹¤ì–‘í•œ í•™ë¶€ ë° ëŒ€í•™ì› í”„ë¡œê·¸ë¨ì„ ì œê³µí•©ë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” íŠ¹íˆ ë²•í•™, ê²½ì œí•™, ì •ì¹˜í•™, ì‚¬íšŒí•™, ë¬¸í•™, ê³¼í•™ ë¶„ì•¼ì—ì„œ ë†’ì€ ëª…ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ, ìŠ¤í¬ì¸  ë¶„ì•¼ì—ì„œë„ í™œë°œí•œ í™œë™ì„ ë³´ì´ë©°, ëŒ€í•œë¯¼êµ­ ëŒ€í•™ ìŠ¤í¬ì¸ ì—ì„œ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê³ ë ¤ëŒ€í•™êµëŠ” êµ­ì œì ì¸ êµë¥˜ì™€ í˜‘ë ¥ì—ë„ ì ê·¹ì ì´ë©°, ì „ ì„¸ê³„ ë‹¤ì–‘í•œ ëŒ€í•™ê³¼ì˜ í˜‘ë ¥ì„ í†µí•´ ê¸€ë¡œë²Œ ê²½ìŸë ¥ì„ ê°•í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤.
```


## Training Details

### Training Data

- [vicgalle/alpaca-gpt4](https://huggingface.co/datasets/vicgalle/alpaca-gpt4)
- Mixed Korean instruction data (gpt-generated, hand-crafted, etc)
- About 66000+ examples used totally

### Training Procedure

- Trained with fixed system prompt below.

```text
ë‹¹ì‹ ì€ ê³ ë ¤ëŒ€í•™êµ NLP&AI ì—°êµ¬ì‹¤ì—ì„œ ë§Œë“  AI ì±—ë´‡ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì´ë¦„ì€ 'KULLM'ìœ¼ë¡œ, í•œêµ­ì–´ë¡œëŠ” 'êµ¬ë¦„'ì„ ëœ»í•©ë‹ˆë‹¤.
ë‹¹ì‹ ì€ ë¹„ë„ë•ì ì´ê±°ë‚˜, ì„±ì ì´ê±°ë‚˜, ë¶ˆë²•ì ì´ê±°ë‚˜ ë˜ëŠ” ì‚¬íšŒ í†µë…ì ìœ¼ë¡œ í—ˆìš©ë˜ì§€ ì•ŠëŠ” ë°œì–¸ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
ì‚¬ìš©ìì™€ ì¦ê²ê²Œ ëŒ€í™”í•˜ë©°, ì‚¬ìš©ìì˜ ì‘ë‹µì— ê°€ëŠ¥í•œ ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ì‘ë‹µí•¨ìœ¼ë¡œì¨ ìµœëŒ€í•œ ë„ì™€ì£¼ë ¤ê³  ë…¸ë ¥í•©ë‹ˆë‹¤.
ì§ˆë¬¸ì´ ì´ìƒí•˜ë‹¤ë©´, ì–´ë–¤ ë¶€ë¶„ì´ ì´ìƒí•œì§€ ì„¤ëª…í•©ë‹ˆë‹¤. ê±°ì§“ ì •ë³´ë¥¼ ë°œì–¸í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•©ë‹ˆë‹¤.
```

## Evaluation

- Evaluation details such as testing data, metrics are written in [github](https://github.com/nlpai-lab/KULLM).
- Without system prompt used in training phase, KULLM would show lower performance than expect.

### Results

<img src="https://huggingface.co/nlpai-lab/KULLM3/resolve/main/kullm3_instruction_evaluation.png" width=100%>


## Citation

```text
@misc{kullm,
  author = {NLP & AI Lab and Human-Inspired AI research},
  title = {KULLM: Korea University Large Language Model Project},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/nlpai-lab/kullm}},
}
```