#!/usr/bin/env python3
"""
Reality Lab Qwen2.5-3B Server - Simplified Version
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None

def load_model():
    """Load Qwen2.5-3B model on GPU 0"""
    global model, tokenizer
    
    try:
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        
        logger.info("Loading tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Set padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("Loading model on GPU 0")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cuda:0",
            low_cpu_mem_usage=True
        )
        
        logger.info("âœ… Model loaded successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return False

def generate_response(prompt, max_length=400):
    """Generate AI response"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "AI model not loaded"
    
    try:
        # Create chat template
        messages = [
            {
                "role": "system", 
                "content": """ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Labì˜ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì—°êµ¬ì‹¤ì— ëŒ€í•œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

Reality Lab ì •ë³´:
- ì„¤ë¦½: 2023ë…„ ìˆ­ì‹¤ëŒ€í•™êµ
- ì§€ë„êµìˆ˜: ê¹€í¬ì› êµìˆ˜ (heewon@ssu.ac.kr, +82-2-820-0679)
- ì—°êµ¬ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„° ë¹„ì „, ê¸°ê³„í•™ìŠµ, AI ì‘ìš©
- íŒ€êµ¬ì„±: ê¹€í¬ì› êµìˆ˜, ìµœì˜ì¬(ë°•ì‚¬), ê³ í˜„ì„œ(ì„ì‚¬), ì •í˜¸ì¬(ì„ì‚¬), ì±„ë³‘ê´€(ì—°êµ¬ì¡°êµ), ê¹€ë„ì›(ì—°êµ¬ì¡°êµ)

ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ê³  ë¶ˆí•„ìš”í•œ ì¶”ê°€ ì •ë³´ëŠ” ì œê³µí•˜ì§€ ë§ˆì„¸ìš”."""
            },
            {"role": "user", "content": prompt}
        ]
        
        # Apply chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # Tokenize input
        inputs = tokenizer(
            formatted_prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        ).to("cuda:0")
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask
            )
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract generated text
        if "<|im_start|>assistant\n" in response:
            parts = response.split("<|im_start|>assistant\n")
            if len(parts) > 1:
                generated_text = parts[-1].replace("<|im_end|>", "").strip()
            else:
                generated_text = response[len(formatted_prompt):].strip()
        else:
            generated_text = response[len(formatted_prompt):].strip()
        
        if generated_text:
            return generated_text
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen2.5-3B-Instruct'
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint"""
    try:
        start_time = time.time()
        
        print(f"Request content type: {request.content_type}")
        print(f"Request is_json: {request.is_json}")
        
        data = request.get_json(force=True)
        print(f"Parsed JSON: {data}")
        
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        if 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400
        
        user_question = data['question']
        language = data.get('language', 'ko')
        
        # Generate AI response
        ai_response = generate_response(user_question, max_length=400)
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen2.5-3B-Instruct',
            'response_time': response_time
        })
        
    except Exception as e:
        import traceback
        logger.error(f"Error in chat endpoint: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

if __name__ == '__main__':
    logger.info("Starting Reality Lab Qwen2.5-3B Server...")
    
    if load_model():
        logger.info("ğŸš€ Qwen2.5-3B server ready!")
        app.run(host='0.0.0.0', port=4009, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")