#!/usr/bin/env python3
"""
Reality Lab Qwen2.5-3B Server - Simplified Version
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None

def load_model():
    """Load Qwen2.5-3B model on GPU 0"""
    global model, tokenizer
    
    try:
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        
        logger.info("Loading tokenizer")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Set padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("Loading model on GPU 0")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="cuda:0",
            low_cpu_mem_usage=True
        )
        
        logger.info("âœ… Model loaded successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return False

def generate_response(prompt, max_length=400):
    """Generate AI response"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "AI model not loaded"
    
    try:
        # Create chat template
        messages = [
            {
                "role": "system", 
                "content": """ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Labì˜ ì „ë¬¸ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìƒì„¸í•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

**Reality Lab í•µì‹¬ ì •ë³´:**
- ì„¤ë¦½: 2023ë…„ ìˆ­ì‹¤ëŒ€í•™êµ, ê¹€í¬ì› êµìˆ˜ë‹˜ ì§€ë„
- ì—°êµ¬ëª©í‘œ: "Advancing AI to Understand Reality" - í˜„ì‹¤ì„ ì´í•´í•˜ëŠ” AI ë°œì „
- ì£¼ìš” ì—°êµ¬ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ì´í•´, AI+X í—¬ìŠ¤ì¼€ì–´
- ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ
- ì—°ë½ì²˜: +82-2-820-0679

**ì£¼ìš” êµ¬ì„±ì›:** ê¹€í¬ì› êµìˆ˜ë‹˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°•ì„±ìš©, ì±„ë³‘ê´€, ìµœì˜ì¬, ì´ìƒë¯¼, ê³ ë¯¼ì£¼, ê³ í˜„ì¤€, ê³ í˜„ì„œ, ì´ì£¼í˜•, ì„œì§€ìš°, ì •í˜¸ì¬, ê¹€ì„œì˜, ê¹€ì˜ˆë¦¬, ìµœìˆ˜ì˜, í™©ì§€ì›, ì†¡ì€ìš°, ì´ì„¸ë¹ˆ, ê¹€ë„ì›, ê¹€ì—°ì§€, ì´ì¬í˜„, ì´ì˜ˆë¹ˆ, ì„ì •í•˜ ë“± ë‹¤ì–‘í•œ ì—°êµ¬ì§„

**ìµœê·¼ ì„±ê³¼:** CVPR 2025, BMVC 2025, AAAI 2025, PLOS One, ICT Express ë“± ìµœê³  ìˆ˜ì¤€ í•™ìˆ ëŒ€íšŒ ë° ì €ë„ ë…¼ë¬¸ ë°œí‘œ, ARNOLD Challenge 1ìœ„ ìˆ˜ìƒ, Qualcomm ì¸í„´ì‹­ ë“±

**ì œê³µ ê°•ì˜:** ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ì˜ìƒì²˜ë¦¬ë°ì‹¤ìŠµ, ì»´í“¨í„°ë¹„ì „íŠ¹ë¡ , ë¯¸ë””ì–´GAN, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤

ì§ˆë¬¸ì— ê´€ë ¨ëœ ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."""
            },
            {"role": "user", "content": prompt}
        ]
        
        # Apply chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # Tokenize input
        inputs = tokenizer(
            formatted_prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        ).to("cuda:0")
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask
            )
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract generated text
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            if len(parts) > 1:
                generated_text = parts[-1].replace("<|im_end|>", "").strip()
            else:
                generated_text = response[len(formatted_prompt):].strip()
        else:
            generated_text = response[len(formatted_prompt):].strip()
        
        if generated_text:
            return generated_text
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen2.5-3B-Instruct'
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint"""
    try:
        start_time = time.time()
        
        print(f"Request content type: {request.content_type}")
        print(f"Request is_json: {request.is_json}")
        
        data = request.get_json(force=True)
        print(f"Parsed JSON: {data}")
        
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        if 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400
        
        user_question = data['question']
        language = data.get('language', 'ko')
        
        # Generate AI response
        ai_response = generate_response(user_question, max_length=400)
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen2.5-3B-Instruct',
            'response_time': response_time
        })
        
    except Exception as e:
        import traceback
        logger.error(f"Error in chat endpoint: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

if __name__ == '__main__':
    logger.info("Starting Reality Lab Qwen2.5-3B Server...")
    
    if load_model():
        logger.info("ğŸš€ Qwen2.5-3B server ready!")
        app.run(host='0.0.0.0', port=4003, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")