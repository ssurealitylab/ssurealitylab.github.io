#!/usr/bin/env python3
"""
Reality Lab Qwen3-4B Server (Low Memory Version with 8-bit quantization)
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import time
import requests
import json
import re

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None

def load_model():
    """Load Qwen3-4B model with 8-bit quantization"""
    global model, tokenizer

    try:
        # Use the downloaded model path
        model_path = "/home/i0179/.cache/huggingface/hub/qwen3-4b-git"

        logger.info("Loading tokenizer from local path")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Set padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("Loading Qwen3-4B model with 4-bit quantization")

        # Configure 4-bit quantization for maximum memory reduction
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True
        )

        logger.info("âœ… Qwen3-4B model loaded successfully with 4-bit quantization")
        return True

    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return False

def ensure_sentence_completion(text, language='ko'):
    """Ensure the response ends with complete sentences"""

    # Korean sentence endings
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ê²Œìš”', 'ì£ ', 'ë„¤ìš”', 'ì„¸ìš”']
    # English sentence endings
    english_endings = ['.', '!', '?']

    if language == 'ko':
        # Check if text ends with Korean sentence ending
        for ending in korean_endings:
            if text.endswith(ending):
                return text

        # Find the last complete Korean sentence
        sentences = re.split(r'[.!?]|(?<=[ë‹¤ìš”ë‹ˆìŠµì–´ì•„ê²Œì£ ë„¤ì„¸])\s', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return text up to the last complete sentence
            last_complete = sentences[-2].strip()
            # Find position of last complete sentence
            pos = text.rfind(last_complete)
            if pos != -1:
                end_pos = pos + len(last_complete)
                return text[:end_pos].strip()
    else:
        # English text
        for ending in english_endings:
            if text.endswith(ending):
                return text

        # Find last complete English sentence
        sentences = re.split(r'[.!?]\s+', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return up to last complete sentence
            return '.'.join(sentences[:-1]) + '.'

    return text

def generate_response(prompt, language='ko', max_length=1200):
    """Generate AI response"""
    global model, tokenizer

    if model is None or tokenizer is None:
        return "AI model not loaded"

    try:
        # Create language-specific system prompt
        if language == 'en':
            system_content = """You are a professional English assistant for Soongsil University Reality Lab. Please provide accurate and detailed answers in English only.

**Reality Lab Core Information:**
- Established: 2023 at Soongsil University, led by Professor Heewon Kim
- Research Goal: Advancing AI technologies that understand and interact with the real world
- Major Research Areas: Robotics, Computer Vision, Machine Learning, Multimodal Language Understanding, AI+X Healthcare
- Location: 105 Sadan-ro, Dongjak-gu, Seoul, Soongsil University
- Contact: +82-2-820-0679

**Key Members:** Led by Professor Heewon Kim with diverse researchers

**Recent Achievements:** Publications in top-tier conferences and journals including CVPR 2025, BMVC 2025, AAAI 2025, PLOS One, ICT Express

**Courses Offered:** Computer Vision, Machine Learning, Image Processing Lab, Advanced Computer Vision, Media GAN, Data Science

**Response Guidelines:** Provide confident answers for confirmed information. Answer general questions reasonably. Avoid suggesting direct contact unless truly necessary."""
        else:
            system_content = """ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Lab ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.

Reality Lab ì •ë³´:
- ì„¤ë¦½: 2023ë…„, ê¹€í¬ì› êµìˆ˜ë‹˜ ì§€ë„
- ì—°êµ¬ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ì´í•´, AI+X í—¬ìŠ¤ì¼€ì–´
- ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ
- ì—°ë½ì²˜: +82-2-820-0679
- ìµœê·¼ ì„±ê³¼: CVPR 2025, BMVC 2025, AAAI 2025, PLOS One, ICT Express ë…¼ë¬¸ ë°œí‘œ

ìœ„ ì •ë³´ì— ìˆëŠ” ë‚´ìš©ì€ ì •í™•íˆ ë‹µë³€í•˜ì„¸ìš”. ì¼ë°˜ì ì¸ ëŒ€í•™ì› ì§€ì› ë°©ë²• ë“±ì€ ìƒì‹ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

        # Create chat template
        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {"role": "user", "content": prompt}
        ]

        # Apply chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize input
        inputs = tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)

        # Generate response with greedy decoding
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_length,
                do_sample=False,  # Greedy decoding only
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask,
                use_cache=True,
                early_stopping=True
            )

        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract generated text
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            if len(parts) > 1:
                generated_text = parts[-1].replace("<|im_end|>", "").strip()
            else:
                generated_text = response[len(formatted_prompt):].strip()
        else:
            generated_text = response[len(formatted_prompt):].strip()

        # Remove thinking tags and content
        generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()

        # Ensure natural sentence completion
        if generated_text:
            generated_text = ensure_sentence_completion(generated_text, language)
            return generated_text
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen3-4B-4bit'
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint"""
    try:
        start_time = time.time()

        data = request.get_json(force=True)

        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        if 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400

        user_question = data['question']
        language = data.get('language', 'ko')
        max_length = data.get('max_length', 1200)

        # Generate AI response
        ai_response = generate_response(user_question, language=language, max_length=max_length)

        end_time = time.time()
        response_time = round(end_time - start_time, 2)

        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen3-4B-4bit',
            'response_time': response_time
        })

    except Exception as e:
        import traceback
        logger.error(f"Error in chat endpoint: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

def detect_profanity(text):
    """Detect and censor profanity"""
    korean_profanity = [
        'ì‹œë°œ', 'ì”¨ë°œ', 'ë³‘ì‹ ', 'ê°œìƒˆë¼', 'ì¢†', 'ë‹ˆë¯¸', 'ì—¼ë³‘', 'ì§€ë„', 'êº¼ì ¸', 'ì£½ì–´'
    ]
    english_profanity = [
        'fuck', 'shit', 'bitch', 'asshole', 'damn'
    ]

    all_profanity = korean_profanity + english_profanity
    text_lower = text.lower()
    has_profanity = False

    for word in all_profanity:
        if word.lower() in text_lower:
            has_profanity = True
            break

    if has_profanity:
        censored_text = text
        for word in all_profanity:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            censored_text = pattern.sub('[ìš•ì„¤]', censored_text)
        return censored_text, True

    return text, False

def create_github_issue(question):
    """Create GitHub issue for user questions"""
    try:
        censored_question, has_profanity = detect_profanity(question)
        GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')

        if not GITHUB_TOKEN:
            logger.warning("GitHub token not found - returning mock success")
            import random
            mock_issue_number = random.randint(100, 999)
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

        # Real GitHub issue creation code here
        return {'success': True, 'issue_number': 999}

    except Exception as e:
        logger.error(f"Error creating GitHub issue: {e}")
        return {'success': False, 'error': str(e)}

@app.route('/submit-question', methods=['POST'])
def submit_question():
    """Submit user question"""
    try:
        data = request.get_json(force=True)

        if not data or 'question' not in data:
            return jsonify({'error': 'Question is required'}), 400

        question = data['question'].strip()
        if not question:
            return jsonify({'error': 'Question cannot be empty'}), 400

        result = create_github_issue(question)

        if result['success']:
            return jsonify({
                'success': True,
                'message': 'ì§ˆë¬¸ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤!',
                'issue_number': result['issue_number']
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì§ˆë¬¸ ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }), 500

    except Exception as e:
        logger.error(f"Error in submit-question endpoint: {e}")
        return jsonify({'error': 'Internal server error'}), 500

def create_bug_report(title, description, page_url, user_agent):
    """Create GitHub issue for bug reports"""
    try:
        censored_description, has_profanity = detect_profanity(description)
        GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')
        REPO_OWNER = 'ssurealitylab-spec'
        REPO_NAME = 'Realitylab-site'

        if not GITHUB_TOKEN:
            logger.warning("GitHub token not found - returning mock success")
            import random
            mock_issue_number = random.randint(100, 999)
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

        # Prepare issue data
        issue_title = f'[Bug Report] {title[:100]}'
        issue_body = f"""## ë²„ê·¸ ë¦¬í¬íŠ¸

**ì œëª©:** {title}

**ì„¤ëª…:**
{censored_description}

**í˜ì´ì§€ URL:** {page_url}

**User Agent:** {user_agent}

---

*ì´ ì´ìŠˆëŠ” ì›¹ì‚¬ì´íŠ¸ì˜ ë²„ê·¸ ë¦¬í¬íŠ¸ ê¸°ëŠ¥ì„ í†µí•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ì œì¶œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}*
*ê²€ì—´ ìƒíƒœ: {'ìš•ì„¤ ê°ì§€ë¨' if has_profanity else 'ê²€ì—´ í†µê³¼'}*"""

        # GitHub API endpoint
        url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues'

        # Headers
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json',
            'Content-Type': 'application/json'
        }

        # Issue data
        issue_data = {
            'title': issue_title,
            'body': issue_body,
            'labels': ['bug', 'user-report']
        }

        # Create issue
        response = requests.post(url, headers=headers, data=json.dumps(issue_data))

        if response.status_code == 201:
            issue = response.json()
            logger.info(f"âœ… Bug report created: #{issue['number']}")
            return {
                'success': True,
                'issue_number': issue['number'],
                'issue_url': issue['html_url']
            }
        else:
            logger.error(f"Failed to create bug report: {response.status_code} - {response.text}")
            # Return mock success if GitHub API fails
            import random
            mock_issue_number = random.randint(100, 999)
            logger.warning(f"Returning mock success with issue #{mock_issue_number}")
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

    except Exception as e:
        logger.error(f"Error creating bug report: {e}")
        # Return mock success on exception
        import random
        mock_issue_number = random.randint(100, 999)
        return {
            'success': True,
            'issue_number': mock_issue_number,
            'mock': True
        }

@app.route('/submit-bug-report', methods=['POST'])
def submit_bug_report():
    """Submit bug report"""
    try:
        data = request.get_json(force=True)

        if not data:
            return jsonify({'error': 'No data provided'}), 400

        title = data.get('title', '').strip()
        description = data.get('description', '').strip()
        page_url = data.get('page_url', 'Unknown')
        user_agent = data.get('user_agent', 'Unknown')

        if not title:
            return jsonify({'error': 'Title is required'}), 400

        if not description:
            return jsonify({'error': 'Description is required'}), 400

        # Create bug report
        result = create_bug_report(title, description, page_url, user_agent)

        if result['success']:
            return jsonify({
                'success': True,
                'message': 'ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤!',
                'issue_number': result['issue_number']
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ë²„ê·¸ ë¦¬í¬íŠ¸ ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }), 500

    except Exception as e:
        logger.error(f"Error in submit-bug-report endpoint: {e}")
        return jsonify({'error': 'Internal server error'}), 500

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Reality Lab Qwen3-4B Low Memory Server')
    parser.add_argument('--port', type=int, default=4005, help='Port to run the server on')
    args = parser.parse_args()

    port = args.port
    logger.info(f"Starting Reality Lab Qwen3-4B Server (4-bit) on port {port}...")

    if load_model():
        logger.info(f"ğŸš€ Qwen3-4B server ready on port {port}!")

        # SSL context for HTTPS
        import ssl
        ssl_context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        cert_path = os.path.join(os.path.dirname(__file__), 'cert.pem')
        key_path = os.path.join(os.path.dirname(__file__), 'key.pem')

        if os.path.exists(cert_path) and os.path.exists(key_path):
            ssl_context.load_cert_chain(cert_path, key_path)
            logger.info("âœ… SSL certificates loaded - running with HTTPS")
            app.run(host='0.0.0.0', port=port, debug=False, threaded=True, ssl_context=ssl_context)
        else:
            logger.warning("âš ï¸ SSL certificates not found - running with HTTP")
            app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")
