#!/usr/bin/env python3
"""
Reality Lab Qwen3-4B Server (Low Memory Version with 8-bit quantization)
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TextIteratorStreamer
from flask import Flask, request, jsonify, Response, stream_with_context
from flask_cors import CORS
import logging
import time
import requests
import json
import re
from threading import Thread
from rag_retriever import RAGRetriever

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None
rag_retriever = None

def load_model(model_choice='qwen3-4b'):
    """Load Qwen model with 4-bit quantization

    Args:
        model_choice: 'qwen3-4b' or 'qwen25-3b'
    """
    global model, tokenizer

    try:
        # Select model path based on choice
        model_paths = {
            'qwen3-4b': "/home/i0179/.cache/huggingface/hub/qwen3-4b-git",
            'qwen25-3b': "/home/i0179/.cache/huggingface/hub/qwen25-3b-git"
        }

        model_names = {
            'qwen3-4b': "Qwen3-4B",
            'qwen25-3b': "Qwen2.5-3B"
        }

        if model_choice not in model_paths:
            raise ValueError(f"Invalid model choice: {model_choice}. Choose from {list(model_paths.keys())}")

        model_path = model_paths[model_choice]
        model_name = model_names[model_choice]

        logger.info(f"Loading {model_name} tokenizer from local path")
        tokenizer = AutoTokenizer.from_pretrained(model_path)

        # Set padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info(f"Loading {model_name} model with 4-bit quantization")

        # Configure 4-bit quantization for maximum memory reduction
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_quant_type="nf4"
        )

        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            quantization_config=quantization_config,
            device_map="auto",
            low_cpu_mem_usage=True
        )

        logger.info(f"âœ… {model_name} model loaded successfully with 4-bit quantization")
        return True

    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return False

def contains_chinese(text):
    """Check if text contains Chinese characters"""
    # Chinese Unicode ranges: \u4e00-\u9fff (CJK Unified Ideographs)
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]')
    matches = chinese_pattern.findall(text)
    # Consider it Chinese if there are more than 5 Chinese characters
    return len(matches) > 5

def ensure_sentence_completion(text, language='ko'):
    """Ensure the response ends with complete sentences"""

    # Korean sentence endings
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ê²Œìš”', 'ì£ ', 'ë„¤ìš”', 'ì„¸ìš”']
    # English sentence endings
    english_endings = ['.', '!', '?']

    if language == 'ko':
        # Check if text ends with Korean sentence ending
        for ending in korean_endings:
            if text.endswith(ending):
                return text

        # Find the last complete Korean sentence
        sentences = re.split(r'[.!?]|(?<=[ë‹¤ìš”ë‹ˆìŠµì–´ì•„ê²Œì£ ë„¤ì„¸])\s', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return text up to the last complete sentence
            last_complete = sentences[-2].strip()
            # Find position of last complete sentence
            pos = text.rfind(last_complete)
            if pos != -1:
                end_pos = pos + len(last_complete)
                return text[:end_pos].strip()
    else:
        # English text
        for ending in english_endings:
            if text.endswith(ending):
                return text

        # Find last complete English sentence
        sentences = re.split(r'[.!?]\s+', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return up to last complete sentence
            return '.'.join(sentences[:-1]) + '.'

    return text

def generate_response(prompt, language='ko', max_length=700):
    """Generate AI response"""
    global model, tokenizer, rag_retriever

    if model is None or tokenizer is None:
        return "AI model not loaded"

    try:
        # Search for relevant context using RAG
        rag_context = ""
        if rag_retriever is not None:
            try:
                search_results = rag_retriever.search(prompt, k=2, min_score=0.35)
                if search_results:
                    rag_context = rag_retriever.format_context(search_results, language=language)
                    rag_context += "\n\n"
            except Exception as e:
                logger.warning(f"RAG search failed: {e}")

        # Create language-specific system prompt (concise yet friendly with key info)
        if language == 'en':
            system_content = f"""{rag_context}You are a helpful assistant for Reality Lab at Soongsil University. Be concise yet friendly, and include all essential information.

Use the reference materials above to answer the question. If the references don't contain the answer, use your general knowledge about Reality Lab.

Reality Lab (Soongsil University):
- Established 2023, Led by Prof. Heewon Kim
- Research: Robotics, Computer Vision, Machine Learning, Multimodal AI, Healthcare AI
- Location: 105 Sadan-ro, Dongjak-gu, Seoul
- Contact: +82-2-820-0679

Guidelines:
- Be concise yet polite and friendly
- Include all key information needed
- Use natural, complete sentences
- No <think> tags or internal reasoning"""
        else:
            system_content = f"""{rag_context}ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Labì˜ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ë©´ì„œë„ ì¹œì ˆí•˜ê²Œ, í•µì‹¬ ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

ìœ„ì˜ ì°¸ê³ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ì°¸ê³ ìë£Œì— ë‹µë³€ì´ ì—†ìœ¼ë©´, Reality Labì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§€ì‹ì„ í™œìš©í•˜ì„¸ìš”.

Reality Lab ê¸°ë³¸ ì •ë³´:
- ì„¤ë¦½: 2023ë…„, ê¹€í¬ì› êµìˆ˜ë‹˜
- ì—°êµ¬ ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ AI, í—¬ìŠ¤ì¼€ì–´ AI
- ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ
- ì—°ë½ì²˜: +82-2-820-0679

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
- **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”** (ì¤‘êµ­ì–´, ì˜ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ ì‚¬ìš© ê¸ˆì§€)
- ê°„ê²°í•˜ë©´ì„œë„ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- í•„ìš”í•œ í•µì‹¬ ì •ë³´ëŠ” ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”
- <think> íƒœê·¸ë‚˜ ë‚´ë¶€ ì¶”ë¡  ê³¼ì •ì€ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”"""

        # Create chat template
        messages = [
            {
                "role": "system",
                "content": system_content
            },
            {"role": "user", "content": prompt}
        ]

        # Apply chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        # Tokenize input
        inputs = tokenizer(
            formatted_prompt,
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(model.device)

        # Generate response with optimized parameters for speed
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_length,
                min_new_tokens=50,  # Ensure substantial response
                do_sample=True,  # Enable sampling for better language control
                temperature=0.3,  # Low temperature for more focused responses
                top_p=0.85,  # Nucleus sampling
                num_beams=1,  # No beam search (fastest)
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask,
                use_cache=True,  # Use KV cache for speed
                early_stopping=True,
                repetition_penalty=1.15  # Higher penalty to prevent language mixing
            )

        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Calculate generated token count
        input_length = inputs.input_ids.shape[1]
        output_length = outputs.shape[1]
        generated_tokens = output_length - input_length

        # Extract generated text
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            if len(parts) > 1:
                generated_text = parts[-1].replace("<|im_end|>", "").strip()
            else:
                generated_text = response[len(formatted_prompt):].strip()
        else:
            generated_text = response[len(formatted_prompt):].strip()

        # Remove thinking tags and content
        generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()

        # Check for Chinese characters (for Korean language mode)
        if language == 'ko' and contains_chinese(generated_text):
            logger.warning("Chinese detected in Korean response, using fallback")
            # Provide Korean fallback message
            if 'ì—°êµ¬' in prompt or 'ë¶„ì•¼' in prompt:
                generated_text = "Reality Labì˜ ì£¼ìš” ì—°êµ¬ ë¶„ì•¼ëŠ” ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„° ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ AI, í—¬ìŠ¤ì¼€ì–´ AIì…ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê¹€í¬ì› êµìˆ˜ë‹˜(heewon@ssu.ac.kr)ê»˜ ë¬¸ì˜í•˜ì‹œë©´ ë©ë‹ˆë‹¤."
            elif 'êµ¬ì„±ì›' in prompt or 'íŒ€' in prompt or 'ë©¤ë²„' in prompt:
                generated_text = "Reality Labì˜ ì£¼ìš” êµ¬ì„±ì›ì€ ê¹€í¬ì› êµìˆ˜ë‹˜(ì„¤ë¦½ì), ì´ì˜ˆë¹ˆ ì¸í„´(ì»´í“¨í„° ë¹„ì „, AI for Sports ì—°êµ¬) ë“±ì…ë‹ˆë‹¤. ìì„¸í•œ êµ¬ì„±ì› ì •ë³´ëŠ” ì—°êµ¬ì‹¤ì— ë¬¸ì˜í•´ì£¼ì„¸ìš”."
            elif 'ì—°ë½' in prompt or 'ì´ë©”ì¼' in prompt or 'ì „í™”' in prompt:
                generated_text = "Reality Lab ì—°ë½ì²˜: ì „í™” +82-2-820-0679, ì´ë©”ì¼ heewon@ssu.ac.kr, ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105 ìˆ­ì‹¤ëŒ€í•™êµ"
            else:
                generated_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ë‹µë³€ì„ ì¤€ë¹„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ê¹€í¬ì› êµìˆ˜ë‹˜(heewon@ssu.ac.kr, +82-2-820-0679)ê»˜ ë¬¸ì˜í•˜ì‹œë©´ ë©ë‹ˆë‹¤."

        # Ensure natural sentence completion
        if generated_text:
            generated_text = ensure_sentence_completion(generated_text, language)
            return generated_text, generated_tokens
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", generated_tokens

    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", 0

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen3-4B-4bit'
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint"""
    try:
        start_time = time.time()

        data = request.get_json(force=True)

        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400

        if 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400

        user_question = data['question']
        language = data.get('language', 'ko')
        max_length = data.get('max_length', 700)  # Balanced default

        # Generate AI response
        ai_response, token_count = generate_response(user_question, language=language, max_length=max_length)

        end_time = time.time()
        response_time = round(end_time - start_time, 2)

        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen3-4B-4bit',
            'response_time': response_time,
            'tokens': token_count
        })

    except Exception as e:
        import traceback
        logger.error(f"Error in chat endpoint: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

@app.route('/chat/stream', methods=['POST'])
def chat_stream():
    """Streaming chat endpoint using Server-Sent Events"""
    try:
        data = request.get_json(force=True)

        if not data or 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400

        user_question = data['question']
        language = data.get('language', 'ko')
        max_length = data.get('max_length', 700)

        def generate_stream():
            global model, tokenizer

            if model is None or tokenizer is None:
                yield f"data: {json.dumps({'error': 'AI model not loaded'})}\n\n"
                return

            try:
                start_time = time.time()

                # Create system prompt
                if language == 'en':
                    system_content = """You are a helpful assistant for Reality Lab at Soongsil University. Be concise yet friendly, and include all essential information.

Reality Lab (Soongsil University):
- Established 2023, Led by Prof. Heewon Kim
- Research: Robotics, Computer Vision, Machine Learning, Multimodal AI, Healthcare AI
- Location: 105 Sadan-ro, Dongjak-gu, Seoul
- Contact: +82-2-820-0679
- Recent Publications: CVPR 2025 (DynScene), BMVC 2025, AAAI 2025, PLOS ONE, ICT Express

Guidelines:
- Be concise yet polite and friendly
- Include all key information needed
- Use natural, complete sentences
- No <think> tags or internal reasoning"""
                else:
                    system_content = """ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Labì˜ ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ë©´ì„œë„ ì¹œì ˆí•˜ê²Œ, í•µì‹¬ ì •ë³´ëŠ” ëª¨ë‘ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

Reality Lab ì •ë³´:
- ì„¤ë¦½: 2023ë…„, ê¹€í¬ì› êµìˆ˜ë‹˜
- ì—°êµ¬ ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ AI, í—¬ìŠ¤ì¼€ì–´ AI
- ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ
- ì—°ë½ì²˜: +82-2-820-0679
- ìµœê·¼ ë…¼ë¬¸: CVPR 2025 (DynScene), BMVC 2025, AAAI 2025, PLOS ONE, ICT Express

ë‹µë³€ ê°€ì´ë“œë¼ì¸:
- **ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”** (ì¤‘êµ­ì–´, ì˜ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ ì‚¬ìš© ê¸ˆì§€)
- ê°„ê²°í•˜ë©´ì„œë„ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- í•„ìš”í•œ í•µì‹¬ ì •ë³´ëŠ” ë¹ ì§ì—†ì´ í¬í•¨í•˜ì„¸ìš”
- ìì—°ìŠ¤ëŸ½ê³  ì™„ì „í•œ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”
- <think> íƒœê·¸ë‚˜ ë‚´ë¶€ ì¶”ë¡  ê³¼ì •ì€ í‘œì‹œí•˜ì§€ ë§ˆì„¸ìš”"""

                messages = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_question}
                ]

                formatted_prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )

                inputs = tokenizer(
                    formatted_prompt,
                    return_tensors="pt",
                    truncation=True,
                    max_length=512
                ).to(model.device)

                # Initialize streamer
                streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)

                # Generation parameters
                generation_kwargs = {
                    "input_ids": inputs.input_ids,
                    "max_new_tokens": max_length,
                    "min_new_tokens": 50,
                    "do_sample": True,
                    "temperature": 0.3,
                    "top_p": 0.85,
                    "num_beams": 1,
                    "pad_token_id": tokenizer.eos_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                    "attention_mask": inputs.attention_mask,
                    "use_cache": True,
                    "early_stopping": True,
                    "repetition_penalty": 1.15,
                    "streamer": streamer
                }

                # Start generation in a separate thread
                thread = Thread(target=model.generate, kwargs=generation_kwargs)
                thread.start()

                # Stream the response
                full_response = ""
                token_count = 0
                in_think_block = False

                for text in streamer:
                    # Track <think> blocks for streaming (tags can be split across tokens)
                    if '<think>' in text:
                        in_think_block = True
                        text = text.replace('<think>', '')

                    if '</think>' in text:
                        in_think_block = False
                        text = text.replace('</think>', '')

                    # Skip content inside <think> blocks
                    if not in_think_block and text.strip():
                        full_response += text
                        token_count += 1
                        yield f"data: {json.dumps({'text': text, 'done': False})}\n\n"

                thread.join()

                # Send final message with stats
                end_time = time.time()
                response_time = round(end_time - start_time, 2)
                yield f"data: {json.dumps({'done': True, 'response_time': response_time, 'tokens': token_count})}\n\n"

            except Exception as e:
                logger.error(f"Error in streaming: {e}")
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return Response(stream_with_context(generate_stream()), mimetype='text/event-stream')

    except Exception as e:
        logger.error(f"Error in chat/stream endpoint: {e}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

def detect_profanity(text):
    """Detect and censor profanity"""
    korean_profanity = [
        'ì‹œë°œ', 'ì”¨ë°œ', 'ë³‘ì‹ ', 'ê°œìƒˆë¼', 'ì¢†', 'ë‹ˆë¯¸', 'ì—¼ë³‘', 'ì§€ë„', 'êº¼ì ¸', 'ì£½ì–´'
    ]
    english_profanity = [
        'fuck', 'shit', 'bitch', 'asshole', 'damn'
    ]

    all_profanity = korean_profanity + english_profanity
    text_lower = text.lower()
    has_profanity = False

    for word in all_profanity:
        if word.lower() in text_lower:
            has_profanity = True
            break

    if has_profanity:
        censored_text = text
        for word in all_profanity:
            pattern = re.compile(re.escape(word), re.IGNORECASE)
            censored_text = pattern.sub('[ìš•ì„¤]', censored_text)
        return censored_text, True

    return text, False

def create_github_issue(question):
    """Create GitHub issue for user questions"""
    try:
        censored_question, has_profanity = detect_profanity(question)
        GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')

        if not GITHUB_TOKEN:
            logger.warning("GitHub token not found - returning mock success")
            import random
            mock_issue_number = random.randint(100, 999)
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

        # Real GitHub issue creation code here
        return {'success': True, 'issue_number': 999}

    except Exception as e:
        logger.error(f"Error creating GitHub issue: {e}")
        return {'success': False, 'error': str(e)}

@app.route('/submit-question', methods=['POST'])
def submit_question():
    """Submit user question"""
    try:
        data = request.get_json(force=True)

        if not data or 'question' not in data:
            return jsonify({'error': 'Question is required'}), 400

        question = data['question'].strip()
        if not question:
            return jsonify({'error': 'Question cannot be empty'}), 400

        result = create_github_issue(question)

        if result['success']:
            return jsonify({
                'success': True,
                'message': 'ì§ˆë¬¸ì´ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤!',
                'issue_number': result['issue_number']
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ì§ˆë¬¸ ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }), 500

    except Exception as e:
        logger.error(f"Error in submit-question endpoint: {e}")
        return jsonify({'error': 'Internal server error'}), 500

def create_bug_report(title, description, page_url, user_agent):
    """Create GitHub issue for bug reports"""
    try:
        censored_description, has_profanity = detect_profanity(description)
        GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')
        REPO_OWNER = 'ssurealitylab-spec'
        REPO_NAME = 'Realitylab-site'

        if not GITHUB_TOKEN:
            logger.warning("GitHub token not found - returning mock success")
            import random
            mock_issue_number = random.randint(100, 999)
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

        # Prepare issue data
        issue_title = f'[Bug Report] {title[:100]}'
        issue_body = f"""## ë²„ê·¸ ë¦¬í¬íŠ¸

**ì œëª©:** {title}

**ì„¤ëª…:**
{censored_description}

**í˜ì´ì§€ URL:** {page_url}

**User Agent:** {user_agent}

---

*ì´ ì´ìŠˆëŠ” ì›¹ì‚¬ì´íŠ¸ì˜ ë²„ê·¸ ë¦¬í¬íŠ¸ ê¸°ëŠ¥ì„ í†µí•´ ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ì œì¶œ ì‹œê°„: {time.strftime('%Y-%m-%d %H:%M:%S')}*
*ê²€ì—´ ìƒíƒœ: {'ìš•ì„¤ ê°ì§€ë¨' if has_profanity else 'ê²€ì—´ í†µê³¼'}*"""

        # GitHub API endpoint
        url = f'https://api.github.com/repos/{REPO_OWNER}/{REPO_NAME}/issues'

        # Headers
        headers = {
            'Authorization': f'token {GITHUB_TOKEN}',
            'Accept': 'application/vnd.github.v3+json',
            'Content-Type': 'application/json'
        }

        # Issue data
        issue_data = {
            'title': issue_title,
            'body': issue_body,
            'labels': ['bug', 'user-report']
        }

        # Create issue
        response = requests.post(url, headers=headers, data=json.dumps(issue_data))

        if response.status_code == 201:
            issue = response.json()
            logger.info(f"âœ… Bug report created: #{issue['number']}")
            return {
                'success': True,
                'issue_number': issue['number'],
                'issue_url': issue['html_url']
            }
        else:
            logger.error(f"Failed to create bug report: {response.status_code} - {response.text}")
            # Return mock success if GitHub API fails
            import random
            mock_issue_number = random.randint(100, 999)
            logger.warning(f"Returning mock success with issue #{mock_issue_number}")
            return {
                'success': True,
                'issue_number': mock_issue_number,
                'mock': True
            }

    except Exception as e:
        logger.error(f"Error creating bug report: {e}")
        # Return mock success on exception
        import random
        mock_issue_number = random.randint(100, 999)
        return {
            'success': True,
            'issue_number': mock_issue_number,
            'mock': True
        }

@app.route('/submit-bug-report', methods=['POST'])
def submit_bug_report():
    """Submit bug report"""
    try:
        data = request.get_json(force=True)

        if not data:
            return jsonify({'error': 'No data provided'}), 400

        title = data.get('title', '').strip()
        description = data.get('description', '').strip()
        page_url = data.get('page_url', 'Unknown')
        user_agent = data.get('user_agent', 'Unknown')

        if not title:
            return jsonify({'error': 'Title is required'}), 400

        if not description:
            return jsonify({'error': 'Description is required'}), 400

        # Create bug report
        result = create_bug_report(title, description, page_url, user_agent)

        if result['success']:
            return jsonify({
                'success': True,
                'message': 'ë²„ê·¸ ë¦¬í¬íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì œì¶œë˜ì—ˆìŠµë‹ˆë‹¤!',
                'issue_number': result['issue_number']
            })
        else:
            return jsonify({
                'success': False,
                'error': 'ë²„ê·¸ ë¦¬í¬íŠ¸ ì œì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.'
            }), 500

    except Exception as e:
        logger.error(f"Error in submit-bug-report endpoint: {e}")
        return jsonify({'error': 'Internal server error'}), 500

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Reality Lab Qwen Low Memory Server')
    parser.add_argument('--port', type=int, default=4005, help='Port to run the server on')
    parser.add_argument('--model', type=str, default='qwen3-4b',
                       choices=['qwen3-4b', 'qwen25-3b'],
                       help='Model to use: qwen3-4b (default) or qwen25-3b')
    args = parser.parse_args()

    port = args.port
    model_choice = args.model

    model_display_names = {
        'qwen3-4b': 'Qwen3-4B',
        'qwen25-3b': 'Qwen2.5-3B'
    }

    logger.info(f"Starting Reality Lab {model_display_names[model_choice]} Server (4-bit) on port {port}...")

    if load_model(model_choice):
        # Load RAG system
        try:
            logger.info("Loading RAG system...")
            rag_retriever = RAGRetriever("/home/i0179/Realitylab-site/ai_server/vector_db")
            rag_retriever.load()
            logger.info("âœ… RAG system loaded successfully!")
        except Exception as e:
            logger.warning(f"âš ï¸ Failed to load RAG system: {e}")
            logger.warning("âš ï¸ Continuing without RAG (will use basic knowledge only)")
            rag_retriever = None

        logger.info(f"ğŸš€ {model_display_names[model_choice]} server ready on port {port}!")
        logger.info("âœ… Running with HTTP (no SSL)")
        app.run(host='0.0.0.0', port=port, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")
