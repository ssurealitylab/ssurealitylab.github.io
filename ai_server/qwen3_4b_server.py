#!/usr/bin/env python3
"""
Reality Lab Qwen3-4B Server
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

# Global variables
model = None
tokenizer = None

def load_model():
    """Load Qwen3-4B model"""
    global model, tokenizer
    
    try:
        # Use the downloaded model path
        model_path = "/home/i0179/.cache/huggingface/hub/qwen3-4b-git"
        
        logger.info("Loading tokenizer from local path")
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # Set padding token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("Loading Qwen3-4B model on GPU 2")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="cuda:0",  # CUDA_VISIBLE_DEVICES=2 makes GPU 2 appear as cuda:0
            low_cpu_mem_usage=True
        )
        
        logger.info("âœ… Qwen3-4B model loaded successfully")
        return True
        
    except Exception as e:
        logger.error(f"Failed to load model: {e}")
        return False

def ensure_sentence_completion(text, language='ko'):
    """Ensure the response ends with complete sentences"""
    import re
    
    # Korean sentence endings
    korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì–´ìš”', 'ì•„ìš”', 'ê²Œìš”', 'ì£ ', 'ë„¤ìš”', 'ì„¸ìš”']
    # English sentence endings
    english_endings = ['.', '!', '?']
    
    if language == 'ko':
        # Check if text ends with Korean sentence ending
        for ending in korean_endings:
            if text.endswith(ending):
                return text
        
        # Find the last complete Korean sentence
        sentences = re.split(r'[.!?]|(?<=[ë‹¤ìš”ë‹ˆìŠµì–´ì•„ê²Œì£ ë„¤ì„¸])\s', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return text up to the last complete sentence
            last_complete = sentences[-2].strip()
            # Find position of last complete sentence
            pos = text.rfind(last_complete)
            if pos != -1:
                end_pos = pos + len(last_complete)
                return text[:end_pos].strip()
    else:
        # English text
        for ending in english_endings:
            if text.endswith(ending):
                return text
        
        # Find last complete English sentence
        sentences = re.split(r'[.!?]\s+', text)
        if len(sentences) > 1 and sentences[-2].strip():
            # Return up to last complete sentence
            return '.'.join(sentences[:-1]) + '.'
    
    return text

def generate_response(prompt, language='ko', max_length=800):
    """Generate AI response"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "AI model not loaded"
    
    try:
        # Create language-specific system prompt
        if language == 'en':
            system_content = """You are a professional English assistant for Soongsil University Reality Lab. Please provide accurate and detailed answers in English only.

**Reality Lab Core Information:**
- Established: 2023 at Soongsil University, led by Professor Heewon Kim
- Research Goal: Advancing AI technologies that understand and interact with the real world
- Major Research Areas: Robotics, Computer Vision, Machine Learning, Multimodal Language Understanding, AI+X Healthcare
- Location: 105 Sadan-ro, Dongjak-gu, Seoul, Soongsil University
- Contact: +82-2-820-0679

**Key Members:** Led by Professor Heewon Kim with diverse researchers including Sungyong Park, Byungkwan Chae, Youngjae Choi, Sangmin Lee, Minju Ko, Hyunjun Ko, Hyunsuh Ko, Juhyeong Lee, Jiwoo Seo, Hojae Jeong, Seoyoung Kim, Yeri Kim, Suyoung Choi, Jiwon Hwang, Eunwoo Song, Sebin Lee, Dowon Kim, Yeonji Kim, Jaehyun Lee, Yebin Lee, Jungha Lim, and others

**Recent Achievements:** Publications in top-tier conferences and journals including CVPR 2025, BMVC 2025, AAAI 2025, PLOS One, ICT Express, ARNOLD Challenge 1st place winner, Qualcomm internships, etc.

**Courses Offered:** Computer Vision, Machine Learning, Image Processing Lab, Advanced Computer Vision, Media GAN, Data Science

**Important:** Always think and respond in English only. Never use Korean."""
        else:
            system_content = """ë‹¹ì‹ ì€ ìˆ­ì‹¤ëŒ€í•™êµ Reality Labì˜ ì „ë¬¸ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ í•œêµ­ì–´ë¡œë§Œ ìƒê°í•˜ê³  í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**Reality Lab í•µì‹¬ ì •ë³´:**
- ì„¤ë¦½: 2023ë…„ ìˆ­ì‹¤ëŒ€í•™êµ, ê¹€í¬ì› êµìˆ˜ë‹˜ ì§€ë„
- ì—°êµ¬ëª©í‘œ: í˜„ì‹¤ì„ ì´í•´í•˜ëŠ” AI ê¸°ìˆ  ë°œì „
- ì£¼ìš” ì—°êµ¬ë¶„ì•¼: ë¡œë³´í‹±ìŠ¤, ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ì´í•´, AI+X í—¬ìŠ¤ì¼€ì–´
- ìœ„ì¹˜: ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ
- ì—°ë½ì²˜: +82-2-820-0679

**ì£¼ìš” êµ¬ì„±ì›:** ê¹€í¬ì› êµìˆ˜ë‹˜ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë°•ì„±ìš©, ì±„ë³‘ê´€, ìµœì˜ì¬, ì´ìƒë¯¼, ê³ ë¯¼ì£¼, ê³ í˜„ì¤€, ê³ í˜„ì„œ, ì´ì£¼í˜•, ì„œì§€ìš°, ì •í˜¸ì¬, ê¹€ì„œì˜, ê¹€ì˜ˆë¦¬, ìµœìˆ˜ì˜, í™©ì§€ì›, ì†¡ì€ìš°, ì´ì„¸ë¹ˆ, ê¹€ë„ì›, ê¹€ì—°ì§€, ì´ì¬í˜„, ì´ì˜ˆë¹ˆ, ì„ì •í•˜ ë“± ë‹¤ì–‘í•œ ì—°êµ¬ì§„

**ìµœê·¼ ì„±ê³¼:** CVPR 2025, BMVC 2025, AAAI 2025, PLOS One, ICT Express ë“± ìµœê³  ìˆ˜ì¤€ í•™ìˆ ëŒ€íšŒ ë° ì €ë„ ë…¼ë¬¸ ë°œí‘œ, ARNOLD Challenge 1ìœ„ ìˆ˜ìƒ, Qualcomm ì¸í„´ì‹­ ë“±

**ì œê³µ ê°•ì˜:** ì»´í“¨í„°ë¹„ì „, ê¸°ê³„í•™ìŠµ, ì˜ìƒì²˜ë¦¬ë°ì‹¤ìŠµ, ì»´í“¨í„°ë¹„ì „íŠ¹ë¡ , ë¯¸ë””ì–´GAN, ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤

**ì¤‘ìš”:** í•œêµ­ì–´ ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ìƒê°í•˜ê³  í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì ˆëŒ€ ì˜ì–´ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""

        # Create chat template
        messages = [
            {
                "role": "system", 
                "content": system_content
            },
            {"role": "user", "content": prompt}
        ]
        
        # Apply chat template
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # Tokenize input
        inputs = tokenizer(
            formatted_prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        ).to("cuda:0")
        
        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=max_length,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                attention_mask=inputs.attention_mask
            )
        
        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract generated text
        if "assistant\n" in response:
            parts = response.split("assistant\n")
            if len(parts) > 1:
                generated_text = parts[-1].replace("<|im_end|>", "").strip()
            else:
                generated_text = response[len(formatted_prompt):].strip()
        else:
            generated_text = response[len(formatted_prompt):].strip()
        
        # Remove thinking tags and content
        import re
        generated_text = re.sub(r'<think>.*?</think>', '', generated_text, flags=re.DOTALL).strip()
        
        # Ensure natural sentence completion
        if generated_text:
            generated_text = ensure_sentence_completion(generated_text, language)
            return generated_text
        else:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        logger.error(f"Error generating response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/health', methods=['GET'])
def health_check():
    """Health check endpoint"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen3-4B'
    })

@app.route('/chat', methods=['POST'])
def chat():
    """Chat endpoint"""
    try:
        start_time = time.time()
        
        print(f"Request content type: {request.content_type}")
        print(f"Request is_json: {request.is_json}")
        
        data = request.get_json(force=True)
        print(f"Parsed JSON: {data}")
        
        if not data:
            return jsonify({'error': 'No JSON data provided'}), 400
        
        if 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400
        
        user_question = data['question']
        language = data.get('language', 'ko')
        
        # Generate AI response
        ai_response = generate_response(user_question, language=language, max_length=800)
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen3-4B',
            'response_time': response_time
        })
        
    except Exception as e:
        import traceback
        logger.error(f"Error in chat endpoint: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

if __name__ == '__main__':
    logger.info("Starting Reality Lab Qwen3-4B Server...")
    
    if load_model():
        logger.info("ğŸš€ Qwen3-4B server ready on port 4004!")
        app.run(host='0.0.0.0', port=4004, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")