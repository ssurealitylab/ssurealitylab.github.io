#!/usr/bin/env python3
"""
True RAG Server for Reality Lab
Real retrieval-augmented generation that produces varied responses
"""

import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from flask import Flask, request, jsonify
from flask_cors import CORS
import logging
import time
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)

model = None
tokenizer = None

# Reality Lab Knowledge Base - Raw facts for retrieval
REALITY_LAB_KNOWLEDGE = {
    "basic_info": {
        "name": "Reality Lab",
        "establishment": "2023ë…„ ìˆ­ì‹¤ëŒ€í•™êµì—ì„œ ì„¤ë¦½",
        "director": "ê¹€í¬ì› êµìˆ˜ë‹˜ì´ ì§€ë„",
        "mission": "Advancing AI to Understand Reality - í˜„ì‹¤ì„ ì´í•´í•˜ëŠ” AI ë°œì „",
        "contact": "+82-2-820-0679",
        "address": "ì„œìš¸íŠ¹ë³„ì‹œ ë™ì‘êµ¬ ì‚¬ë‹¹ë¡œ 105, ìˆ­ì‹¤ëŒ€í•™êµ"
    },
    "research_areas": [
        "ë¡œë³´í‹±ìŠ¤ (Robotics) - ë¡œë´‡ ê¸°ìˆ  ì—°êµ¬",
        "ì»´í“¨í„°ë¹„ì „ (Computer Vision) - ì˜ìƒ ì¸ì‹ ë° ì²˜ë¦¬",
        "ê¸°ê³„í•™ìŠµ (Machine Learning) - ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ ì—°êµ¬", 
        "ë©€í‹°ëª¨ë‹¬ ì–¸ì–´ì´í•´ (Multimodal Language Understanding) - ë‹¤ì¤‘ ëª¨ë‹¬ AI",
        "AI+X í—¬ìŠ¤ì¼€ì–´ - ì˜ë£Œ ë¶„ì•¼ AI ì‘ìš©"
    ],
    "members": [
        "ê¹€í¬ì› êµìˆ˜ë‹˜ (ì§€ë„êµìˆ˜)",
        "ë°•ì„±ìš©", "ì±„ë³‘ê´€", "ìµœì˜ì¬", "ì´ìƒë¯¼", "ê³ ë¯¼ì£¼",
        "ê³ í˜„ì¤€", "ê³ í˜„ì„œ", "ì´ì£¼í˜•", "ì„œì§€ìš°", "ì •í˜¸ì¬", 
        "ê¹€ì„œì˜", "ê¹€ì˜ˆë¦¬", "ìµœìˆ˜ì˜", "í™©ì§€ì›", "ì†¡ì€ìš°",
        "ì´ì„¸ë¹ˆ", "ê¹€ë„ì›", "ê¹€ì—°ì§€", "ì´ì¬í˜„", "ì´ì˜ˆë¹ˆ", "ì„ì •í•˜"
    ],
    "achievements": [
        "CVPR 2025 ë…¼ë¬¸ ë°œí‘œ",
        "BMVC 2025 ë…¼ë¬¸ ë°œí‘œ",
        "AAAI 2025 ë…¼ë¬¸ ë°œí‘œ", 
        "PLOS One ì €ë„ ë…¼ë¬¸ ê²Œì¬",
        "ICT Express ì €ë„ ë…¼ë¬¸ ê²Œì¬",
        "ARNOLD Challenge 1ìœ„ ìˆ˜ìƒ",
        "Qualcomm ì¸í„´ì‹­ í”„ë¡œê·¸ë¨ ì°¸ì—¬"
    ],
    "courses": [
        "ì»´í“¨í„°ë¹„ì „ - ì˜ìƒ ì²˜ë¦¬ ê¸°ì´ˆ",
        "ê¸°ê³„í•™ìŠµ - ë¨¸ì‹ ëŸ¬ë‹ ì´ë¡ ê³¼ ì‹¤ìŠµ",
        "ì˜ìƒì²˜ë¦¬ë°ì‹¤ìŠµ - ë””ì§€í„¸ ì˜ìƒ ì²˜ë¦¬",
        "ì»´í“¨í„°ë¹„ì „íŠ¹ë¡  - ê³ ê¸‰ ì»´í“¨í„° ë¹„ì „",
        "ë¯¸ë””ì–´GAN - ìƒì„±í˜• AI",
        "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤ - ë°ì´í„° ë¶„ì„ ë° í™œìš©"
    ]
}

def retrieve_relevant_knowledge(question):
    """Retrieve relevant knowledge pieces based on question"""
    question_lower = question.lower()
    retrieved_facts = []
    
    # Basic info retrieval
    if any(word in question_lower for word in ['ì—°ë½', 'contact', 'ì „í™”', 'ë²ˆí˜¸']):
        retrieved_facts.append(f"ì—°ë½ì²˜: {REALITY_LAB_KNOWLEDGE['basic_info']['contact']}")
        retrieved_facts.append(f"ì£¼ì†Œ: {REALITY_LAB_KNOWLEDGE['basic_info']['address']}")
    
    if any(word in question_lower for word in ['ì„¤ë¦½', 'ì–¸ì œ', 'established', 'êµìˆ˜']):
        retrieved_facts.append(f"ì„¤ë¦½: {REALITY_LAB_KNOWLEDGE['basic_info']['establishment']}")
        retrieved_facts.append(f"ì§€ë„êµìˆ˜: {REALITY_LAB_KNOWLEDGE['basic_info']['director']}")
    
    if any(word in question_lower for word in ['ì—°êµ¬', 'ë¶„ì•¼', 'research']):
        retrieved_facts.extend(REALITY_LAB_KNOWLEDGE['research_areas'])
        retrieved_facts.append(f"ì—°êµ¬ ëª©í‘œ: {REALITY_LAB_KNOWLEDGE['basic_info']['mission']}")
    
    if any(word in question_lower for word in ['êµ¬ì„±ì›', 'ë©¤ë²„', 'member', 'íŒ€', 'team']):
        retrieved_facts.extend(REALITY_LAB_KNOWLEDGE['members'])
    
    if any(word in question_lower for word in ['ì„±ê³¼', 'achievement', 'ë…¼ë¬¸', 'paper']):
        retrieved_facts.extend(REALITY_LAB_KNOWLEDGE['achievements'])
    
    if any(word in question_lower for word in ['ê°•ì˜', 'course', 'ìˆ˜ì—…']):
        retrieved_facts.extend(REALITY_LAB_KNOWLEDGE['courses'])
    
    # Default retrieval if no specific match
    if not retrieved_facts:
        retrieved_facts.extend([
            REALITY_LAB_KNOWLEDGE['basic_info']['establishment'],
            REALITY_LAB_KNOWLEDGE['basic_info']['director'],
            REALITY_LAB_KNOWLEDGE['basic_info']['mission']
        ])
    
    return retrieved_facts

def load_model():
    """Load Qwen2.5-3B model"""
    global model, tokenizer
    
    try:
        model_name = "Qwen/Qwen2.5-3B-Instruct"
        
        logger.info("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        logger.info("Loading model...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        logger.info("âœ… Model loaded successfully!")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error loading model: {e}")
        return False

def generate_rag_response(question):
    """True RAG: Retrieve knowledge and generate varied responses"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
    
    try:
        # Step 1: Check if question is Reality Lab related
        question_lower = question.lower()
        reality_lab_keywords = ['ì—°êµ¬', 'research', 'ë¶„ì•¼', 'êµ¬ì„±ì›', 'ë©¤ë²„', 'member', 'íŒ€', 'team', 
                               'ì—°ë½', 'contact', 'ì „í™”', 'ë²ˆí˜¸', 'ì£¼ì†Œ', 'address', 'ìœ„ì¹˜', 'location',
                               'ì„¤ë¦½', 'established', 'êµìˆ˜', 'professor', 'ì„±ê³¼', 'achievement', 
                               'ë…¼ë¬¸', 'paper', 'ê°•ì˜', 'course', 'ìˆ˜ì—…', 'class', 'reality lab',
                               'ë¦¬ì–¼ë¦¬í‹°ë©', 'ìˆ­ì‹¤', 'ìˆ­ì‹¤ëŒ€', 'ssu']
        
        is_relevant = any(keyword in question_lower for keyword in reality_lab_keywords)
        
        if not is_relevant:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. Reality Lab ì±—ë´‡ì€ ì—°êµ¬ì‹¤ ê´€ë ¨ ì§ˆë¬¸ì—ë§Œ ë‹µë³€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Reality Labì˜ ì—°êµ¬ ë¶„ì•¼, êµ¬ì„±ì›, ì—°ë½ì²˜, ì„±ê³¼ ë“±ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”!"
        
        # Step 2: Retrieve relevant knowledge
        retrieved_facts = retrieve_relevant_knowledge(question)
        
        # Step 3: Create context from retrieved facts
        context = "\n".join(retrieved_facts)
        
        # Step 3: Generate varied prompt with strict Korean language instruction
        system_instruction = "ë‹¹ì‹ ì€ Reality Lab ì „ë¬¸ í•œêµ­ì–´ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í‘œì¤€ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì™¸êµ­ì–´ë‚˜ íŠ¹ìˆ˜ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
        
        prompt_variations = [
            f"{system_instruction}\n\nì§ˆë¬¸: {question}\n\nì°¸ê³  ìë£Œ:\n{context}\n\nìœ„ ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:",
            f"{system_instruction}\n\nì‚¬ìš©ì ì§ˆë¬¸: {question}\n\nê´€ë ¨ ì •ë³´:\n{context}\n\nì´ ì •ë³´ë“¤ì„ í™œìš©í•´ì„œ ê¹”ë”í•œ í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:",
            f"{system_instruction}\n\nì§ˆë¬¸ ë‚´ìš©: {question}\n\nì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì •ë³´:\n{context}\n\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‘œì¤€ í•œêµ­ì–´ë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:"
        ]
        
        prompt = random.choice(prompt_variations)
        
        # Step 4: Tokenize with random length variation
        max_length = random.randint(512, 800)
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
        
        # Step 5: Generate with varied parameters for different responses
        temperature = random.uniform(0.4, 0.7)
        top_p = random.uniform(0.8, 0.95)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=random.randint(100, 250),
                temperature=temperature,
                do_sample=True,
                top_p=top_p,
                repetition_penalty=random.uniform(1.1, 1.3),
                no_repeat_ngram_size=3,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
        
        # Step 6: Decode response
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract generated part
        generated_text = full_response[len(prompt):].strip()
        
        return generated_text if generated_text else "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
    except Exception as e:
        logger.error(f"Error generating RAG response: {e}")
        return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'model_name': 'Qwen2.5-3B True RAG',
        'method': 'True Retrieval-Augmented Generation'
    })

@app.route('/chat', methods=['POST'])
def chat():
    try:
        start_time = time.time()
        
        data = request.get_json(force=True)
        if not data or 'question' not in data:
            return jsonify({'error': 'No question provided'}), 400
        
        user_question = data['question']
        language = data.get('language', 'ko')
        
        # Generate RAG response
        ai_response = generate_rag_response(user_question)
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        return jsonify({
            'response': ai_response,
            'language': language,
            'model': 'Qwen2.5-3B True RAG',
            'method': 'Retrieval-Augmented Generation',
            'response_time': response_time
        })
        
    except Exception as e:
        logger.error(f"Error in chat endpoint: {e}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

@app.route('/generate', methods=['POST'])
def generate():
    try:
        start_time = time.time()
        
        data = request.get_json(force=True)
        if not data or 'prompt' not in data:
            return jsonify({'error': 'No prompt provided'}), 400
        
        user_prompt = data['prompt']
        language = data.get('language', 'ko')
        
        response = generate_rag_response(user_prompt)
        
        end_time = time.time()
        response_time = round(end_time - start_time, 2)
        
        return jsonify({
            'response': response,
            'language': language,
            'model': 'Qwen2.5-3B True RAG',
            'method': 'True RAG',
            'response_time': response_time
        })
        
    except Exception as e:
        logger.error(f"Error in generate endpoint: {e}")
        return jsonify({'error': f'Internal server error: {str(e)}'}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ Starting True RAG Server...")
    
    if load_model():
        logger.info("âœ… True RAG server ready on port 4003!")
        app.run(host='0.0.0.0', port=4003, debug=False, threaded=True)
    else:
        logger.error("âŒ Failed to load model. Server not started.")