---
layout: page
title: International Publications
---

{% include sidl-script.html %}

<!-- Publications Header -->
<section class="publications-header text-center py-5">
  <div class="container">
    <h1 class="section-heading text-uppercase">International Publications</h1>
    <h3 class="section-subheading text-muted">Global research achievements and scholarly contributions</h3>
  </div>
</section>

<!-- Publications Content -->
<section class="publications-content py-5">
  <div class="container">
    
    <!-- International Section -->
    <div class="category-section mb-5 year-2025">
      <div class="row">
        <div class="col-12">
          <div class="category-header d-flex align-items-center mb-4">
            <div class="category-badge international-badge">2025</div>
            <div class="category-line flex-grow-1"></div>
          </div>
        </div>
      </div>

      <div class="row">
        <!-- APP3DV 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c24')" style="cursor: pointer;">
            <div class="publication-badge challenge-badge">APP3DV25</div>
            <div class="external-links">
              <a href="https://drive.google.com/file/d/10vp2SWByAJGr-Ccrw68c2hTepsMr5di9/view" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://oshmos.github.io/IPRF/" target="_blank" class="external-icon-link website-link" title="Website" onclick="event.stopPropagation()">
                <i class="fas fa-globe"></i>
              </a>
              <a href="https://github.com/OSHMOS/IPRF" target="_blank" class="external-icon-link github-link" title="GitHub" onclick="event.stopPropagation()">
                <i class="fab fa-github"></i>
              </a>
            </div>
            <h5 class="list-title">Intrinsic-Guided Photorealistic Style Transfer for Radiance Fields</h5>
            <p class="list-authors">Hyunsuh Koh*, Seunghyun Oh*, Jungyun Jang*, Heewon Kim</p>
          </div>
        </div>

        <!-- BMVC 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c23')" style="cursor: pointer;">
            <div class="publication-badge">BMVC25</div>
            <h5 class="list-title">Unsupervised Discovery of Transformations between Fine-Grained Classes in Diffusion Models</h5>
            <p class="list-authors">Youngjae Choi*, Hyunsuh Koh*, Hojae Jeong*, ByungKwan Chae*, Sungyong Park, Heewon Kim</p>
          </div>
        </div>

        <!-- PLOS ONE 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j12')" style="cursor: pointer;">
            <div class="publication-badge">PLOS ONE</div>
            <div class="external-links">
              <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0324169" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">DeepGAM: An Interpretable Deep Neural Network Using Generalized Additive Model for Depression Diagnosis</h5>
            <p class="list-authors">Chiyoung Lee*, Yeri Kim*†, Seoyoung Kim*†, Mary Whooley, Heewon Kim</p>
          </div>
        </div>

        <!-- JEET 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j11')" style="cursor: pointer;">
            <div class="publication-badge">JEET</div>
            <h5 class="list-title">Dog Cough Sound Classification Using Neural Networks for Diagnosing Bronchial Diseases</h5>
            <p class="list-authors">Do-Ye Kwon*†, Yeon-Ju Oh*†, Heewon Kim</p>
          </div>
        </div>

        <!-- ICT Express 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j10')" style="cursor: pointer;">
            <div class="publication-badge">ICT Express</div>
            <div class="external-links">
              <a href="https://pdf.sciencedirectassets.com/313521/1-s2.0-S2405959525X00047/1-s2.0-S2405959525000360/main.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Accurate Baseball Player Pose Refinement Using Motion Prior Guidance</h5>
            <p class="list-authors">Seunghyun Oh†, Heewon Kim</p>
          </div>
        </div>

        <!-- CVPR 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c22')" style="cursor: pointer;">
            <div class="publication-badge">CVPR25</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://cvpr.thecvf.com/virtual/2025/poster/33953" target="_blank" class="external-icon-link website-link" title="Website" onclick="event.stopPropagation()">
                <i class="fas fa-globe"></i>
              </a>
            </div>
            <h5 class="list-title">DynScene: Scalable Generation of Dynamic Robotic Manipulation Scenes for Embodied AI</h5>
            <p class="list-authors">Sangmin Lee*, Sungyong Park*, Heewon Kim</p>
          </div>
        </div>

        <!-- SIDL - AAAI 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('sidl')" style="cursor: pointer;">
            <div class="publication-badge">AAAI25</div>
            <div class="external-links">
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32257" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://sidl-benchmark.github.io/" target="_blank" class="external-icon-link website-link" title="Website" onclick="event.stopPropagation()">
                <i class="fas fa-globe"></i>
              </a>
            </div>
            <h5 class="list-title">SIDL: A Real-World Dataset for Restoring Smartphone Images with Dirty Lenses</h5>
            <p class="list-authors">Sooyoung Choi*†, Sungyong Park*, Heewon Kim</p>
          </div>
        </div>

        <!-- Brain Stimulation 2025 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c20j9')" style="cursor: pointer;">
            <div class="publication-badge">Brain Stimulation</div>
            <div class="external-links">
              <a href="https://www.brainstimjrnl.com/article/S1935-861X%2824%2900905-7/fulltext" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Predictors of the treatment effects of transcranial direct current stimulation on knee osteoarthritis pain</h5>
            <p class="list-authors">Chiyoung Lee, Heewon Kim, Yeri Kim†, Seoyoung Kim†, et al.</p>
          </div>
        </div>

        <!-- ARNOLD Challenge -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('arnold')" style="cursor: pointer;">
            <div class="publication-badge challenge-badge">ARNOLD Challenge</div>
            <h5 class="list-title">1st Place Winner at CVPR 2025 Embodied AI Workshop</h5>
            <p class="list-authors">Reality Lab Team</p>
          </div>
        </div>
      </div>
    </div>

    <!-- 2024 Year Section -->
    <div class="category-section mb-5 year-2024">
      <div class="row">
        <div class="col-12">
          <div class="category-header d-flex align-items-center mb-4">
            <div class="category-badge international-badge">2024</div>
            <div class="category-line flex-grow-1"></div>
          </div>
        </div>
      </div>

      <div class="row">
        <!-- Brain Stimulation 2024 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c17j8')" style="cursor: pointer;">
            <div class="publication-badge">@</div>
            <h5 class="list-title">Predicting tDCS Treatment Effects in Depression Using Deep Learning</h5>
            <p class="list-authors">Chiyoung Lee, Heewon Kim, Seoyoung Kim†, Yeri Kim†, et al.</p>
          </div>
        </div>

        <!-- JEET 2024 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j7')" style="cursor: pointer;">
            <div class="publication-badge">JEET</div>
            <div class="external-links">
              <a href="https://www.mdpi.com/1422-0067/25/12/6761" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Machine Learning-Based Etiologic Subtyping of Ischemic Stroke Using Circulating Exosomal microRNAs</h5>
            <p class="list-authors">Jeongmoon Kim†, Sooyoung Choi†, Heewon Kim</p>
          </div>
        </div>

        <!-- AAAI 2024 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c16')" style="cursor: pointer;">
            <div class="publication-badge">AAAI24</div>
            <h5 class="list-title">Language-Guided Robotic Manipulation</h5>
            <p class="list-authors">Sangmin Lee*, Sungyong Park*, Heewon Kim</p>
          </div>
        </div>

        <!-- ICT Express 2024 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j6')" style="cursor: pointer;">
            <div class="publication-badge">ICT Express</div>
            <h5 class="list-title">Baseball Player Pose Estimation</h5>
            <p class="list-authors">Seunghyun Oh†, Heewon Kim</p>
          </div>
        </div>
      </div>
    </div>

    <!-- 2023 Year Section -->
    <div class="category-section mb-5 year-2023">
      <div class="row">
        <div class="col-12">
          <div class="category-header d-flex align-items-center mb-4">
            <div class="category-badge international-badge">2023</div>
            <div class="category-line flex-grow-1"></div>
          </div>
        </div>
      </div>

      <div class="row">
        <!-- ICASSP 2023 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c15')" style="cursor: pointer;">
            <div class="publication-badge">@</div>
            <div class="external-links">
              <a href="https://openreview.net/forum?id=NO0ThzteQdI" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">NERDS: A General Framework to Train Camera Denoisers</h5>
            <p class="list-authors">Yunseok Yang†, Heewon Kim</p>
          </div>
        </div>

        <!-- WACV 2023 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c14')" style="cursor: pointer;">
            <div class="publication-badge">@</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Hong_DAQ_Channel-Wise_Distribution-Aware_Quantization_for_Deep_Image_Super-Resolution_Networks_WACV_2022_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">DAQ: Channel-Wise Distribution-Aware Quantization</h5>
            <p class="list-authors">Sungyong Park*, Sangmin Lee*, Hyunsuh Koh*, Heewon Kim</p>
          </div>
        </div>

        <!-- ICRA 2023 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c13')" style="cursor: pointer;">
            <div class="publication-badge">@</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Oh_Batch_Normalization_Tells_You_Which_Filter_Is_Important_WACV_2022_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Batch Normalization Tells You Which Filter is Important</h5>
            <p class="list-authors">Sangmin Lee*, Sungyong Park*, Heewon Kim</p>
          </div>
        </div>

        <!-- Applied Sciences 2023 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j5')" style="cursor: pointer;">
            <div class="publication-badge">Applied Sciences</div>
            <div class="external-links">
              <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272330" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Machine learning-based predictive modeling of depression in hypertensive populations</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>
      </div>
    </div>

    <!-- 2022 Year Section -->
    <div class="category-section mb-5 year-2022">
      <div class="row">
        <div class="col-12">
          <div class="category-header d-flex align-items-center mb-4">
            <div class="category-badge international-badge">2022</div>
            <div class="category-line flex-grow-1"></div>
          </div>
        </div>
      </div>

      <div class="row">
        <!-- ECCV 2022 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c12')" style="cursor: pointer;">
            <div class="publication-badge">ECCV22</div>
            <div class="external-links">
              <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670360.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://github.com/Cheeun/CADyQ" target="_blank" class="external-icon-link github-link" title="GitHub" onclick="event.stopPropagation()">
                <i class="fab fa-github"></i>
              </a>
            </div>
            <h5 class="list-title">CADyQ: Content-Aware Dynamic Quantization</h5>
            <p class="list-authors">Sungyong Park*, Sangmin Lee*, Heewon Kim</p>
          </div>
        </div>

        <!-- Sensors 2022 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j4')" style="cursor: pointer;">
            <div class="publication-badge">Sensors</div>
            <div class="external-links">
              <a href="https://ieeexplore.ieee.org/document/10225702" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Learning Controllable ISP for Image Enhancement</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- CVPR 2022 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c11')" style="cursor: pointer;">
            <div class="publication-badge">CVPR22</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Oh_Attentive_Fine-Grained_Structured_Sparsity_for_Image_Restoration_CVPR_2022_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Attentive Fine-Grained Structured Sparsity for Image Restoration</h5>
            <p class="list-authors">Sangmin Lee*, Sungyong Park*, Heewon Kim</p>
          </div>
        </div>
      </div>
    </div>

    <!-- 2021 and Earlier Section -->
    <div class="category-section mb-5 year-2021">
      <div class="row">
        <div class="col-12">
          <div class="category-header d-flex align-items-center mb-4">
            <div class="category-badge international-badge">2021 & Earlier</div>
            <div class="category-line flex-grow-1"></div>
          </div>
        </div>
      </div>

      <div class="row">
        <!-- ICCV 2021 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c10')" style="cursor: pointer;">
            <div class="publication-badge">ICCV21</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Baik_Meta-Learning_With_Task-Adaptive_Loss_Function_for_Few-Shot_Learning_ICCV_2021_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://github.com/baiksung/MeTAL" target="_blank" class="external-icon-link github-link" title="GitHub" onclick="event.stopPropagation()">
                <i class="fab fa-github"></i>
              </a>
            </div>
            <h5 class="list-title">MeTAL: Meta-Learning with Task-Adaptive Loss Function</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- IEEE Access 2020 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j3')" style="cursor: pointer;">
            <div class="publication-badge">IEEE Access</div>
            <div class="external-links">
              <a href="https://ieeexplore.ieee.org/document/10080995" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Learning to Learn Task-Adaptive Hyperparameters for Few-Shot Learning</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- NeurIPS 2019 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c9')" style="cursor: pointer;">
            <div class="publication-badge">@</div>
            <div class="external-links">
              <a href="https://papers.nips.cc/paper/2020/file/ee89223a2b625b5152132ed77abbcc79-Paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://github.com/baiksung/ALFA" target="_blank" class="external-icon-link github-link" title="GitHub" onclick="event.stopPropagation()">
                <i class="fab fa-github"></i>
              </a>
            </div>
            <h5 class="list-title">ALFA: Meta-Learning with Adaptive Hyperparameters</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- Pattern Recognition 2018 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j2')" style="cursor: pointer;">
            <div class="publication-badge">Pattern Recognition</div>
            <div class="external-links">
              <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320322001742" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Fine-grained neural architecture search for image super-resolution</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- CVPR 2017 Workshop -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('c8')" style="cursor: pointer;">
            <div class="publication-badge workshop-badge">CVPRW17</div>
            <div class="external-links">
              <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
              <a href="https://github.com/sanghyun-son/EDSR-PyTorch" target="_blank" class="external-icon-link github-link" title="GitHub" onclick="event.stopPropagation()">
                <i class="fab fa-github"></i>
              </a>
            </div>
            <h5 class="list-title">EDSR: Enhanced Deep Residual Networks for Single Image Super-Resolution</h5>
            <p class="list-authors">Bee Lim*, Sanghyun Son*, Heewon Kim, et al.</p>
          </div>
        </div>

        <!-- Computer Vision and Image Understanding 2017 -->
        <div class="col-12 mb-3">
          <div class="publication-list-item" onclick="openPublicationModal('j1')" style="cursor: pointer;">
            <div class="publication-badge">CVIU</div>
            <div class="external-links">
              <a href="https://www.dropbox.com/scl/fi/8km8l254msd8rm8cjfkct/AAAI-ChoiM.4773.pdf?rlkey=rws6k9ctw5prhis162zk30xts&dl=0" target="_blank" class="external-icon-link pdf-link" title="PDF" onclick="event.stopPropagation()">
                <i class="fas fa-file-pdf"></i>
              </a>
            </div>
            <h5 class="list-title">Channel Attention Is All You Need for Video Frame Interpolation</h5>
            <p class="list-authors">Heewon Kim, et al.</p>
          </div>
        </div>
      </div>
    </div>
    
  </div>
</section>

<!-- Research Focus -->
<section class="py-5 bg-light">
  <div class="container text-center">
    <div class="row">
      <div class="col-lg-10 mx-auto">
        <h3 class="text-uppercase mb-4">International Research Focus</h3>
        <div class="row">
          <div class="col-md-6">
            <h5>Global Conferences</h5>
            <ul class="list-unstyled text-left">
              <li><i class="fas fa-trophy text-warning"></i> CVPR, BMVC, AAAI</li>
              <li><i class="fas fa-robot text-primary"></i> ICRA, IROS, RSS</li>
              <li><i class="fas fa-brain text-success"></i> NeurIPS, ICML, ICLR</li>
              <li><i class="fas fa-eye text-info"></i> ICCV, ECCV, WACV</li>
            </ul>
          </div>
          <div class="col-md-6">
            <h5>Recent Achievements</h5>
            <ul class="list-unstyled text-left">
              <li><i class="fas fa-trophy text-warning"></i> ARNOLD Challenge 1st Place</li>
              <li><i class="fas fa-file-alt text-info"></i> AAAI 2025 Paper Acceptance</li>
              <li><i class="fas fa-file-alt text-info"></i> CVPR 2025 Paper Acceptance</li>
              <li><i class="fas fa-file-alt text-info"></i> BMVC 2025 Paper Acceptance</li>
            </ul>
          </div>
        </div>
        <hr class="my-4">
        <p class="text-muted">
          <strong>Reality Lab</strong> | Global School of Media, College of IT, Soongsil University<br>
          <strong>Principal Investigator:</strong> Prof. Heewon Kim
        </p>
      </div>
    </div>
  </div>
</section>

<style>
/* Publications Content Styles */
.publications-header {
  background: transparent;
}

/* Category Section */
.category-section {
  margin-bottom: 4rem;
}

.category-header {
  margin-bottom: 2rem;
}

.category-badge {
  color: white;
  padding: 12px 24px;
  border-radius: 50px;
  font-size: 1.3rem;
  font-weight: bold;
  margin-right: 20px;
  min-width: 140px;
  text-align: center;
}

.international-badge {
  background: linear-gradient(135deg, #667eea, #764ba2);
  box-shadow: 0 4px 15px rgba(102, 126, 234, 0.3);
}

.category-line {
  height: 3px;
  background: linear-gradient(90deg, #667eea, rgba(102, 126, 234, 0.1));
  border-radius: 2px;
}

/* Year-specific badge and line colors */
.year-2025 .category-badge {
  background: linear-gradient(135deg, #7a8dd6, #8769a8);
  box-shadow: 0 4px 15px rgba(122, 141, 214, 0.28);
}

.year-2025 .category-line {
  background: linear-gradient(90deg, #7a8dd6, rgba(122, 141, 214, 0.1));
}

.year-2024 .category-badge {
  background: linear-gradient(135deg, #f39c12, #e67e22);
  box-shadow: 0 4px 15px rgba(243, 156, 18, 0.28);
}

.year-2024 .category-line {
  background: linear-gradient(90deg, #f39c12, rgba(243, 156, 18, 0.1));
}

.year-2023 .category-badge {
  background: linear-gradient(135deg, #66b2e4, #4dd5e1);
  box-shadow: 0 4px 15px rgba(102, 178, 228, 0.28);
}

.year-2023 .category-line {
  background: linear-gradient(90deg, #66b2e4, rgba(102, 178, 228, 0.1));
}

.year-2022 .category-badge {
  background: linear-gradient(135deg, #5fd68f, #5be1c7);
  box-shadow: 0 4px 15px rgba(95, 214, 143, 0.28);
}

.year-2022 .category-line {
  background: linear-gradient(90deg, #5fd68f, rgba(95, 214, 143, 0.1));
}

.year-2021 .category-badge {
  background: linear-gradient(135deg, #e788a1, #eccd6d);
  box-shadow: 0 4px 15px rgba(231, 136, 161, 0.28);
}

.year-2021 .category-line {
  background: linear-gradient(90deg, #e788a1, rgba(231, 136, 161, 0.1));
}

/* Publication Cards */
.publication-card {
  border: none;
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 2px 10px rgba(0, 0, 0, 0.08);
  transition: all 0.3s ease;
  height: 100%;
  background: white;
  border: 3px solid transparent;
  position: relative;
  min-height: 120px;
}

.publication-card:hover {
  transform: translateY(-3px);
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.15);
}



.research-card {
  border-color: #9b59b6 !important;
}

.research-card:hover {
  border-color: #bb8fce !important;
  box-shadow: 0 8px 30px rgba(155, 89, 182, 0.3);
}


/* Card Layout - All Uniform */
.publication-card .card-title {
  font-size: 0.95rem;
  font-weight: 600;
  color: #2c3e50;
  margin-bottom: 6px;
  line-height: 1.3;
  min-height: 2.6rem;
  display: flex;
  align-items: flex-start;
}

.publication-card .card-subtitle {
  font-size: 0.85rem;
  color: #7f8c8d;
  margin-bottom: 0;
  font-weight: 500;
  line-height: 1.3;
  word-wrap: break-word;
  overflow-wrap: break-word;
  white-space: normal;
}

/* Publication Badge */
.publication-badge {
  position: absolute;
  top: 8px;
  right: 8px;
  background: transparent;
  color: #4facfe;
  border: 2px solid #4facfe;
  padding: 4px 10px;
  border-radius: 15px;
  font-weight: bold;
  font-size: 0.7rem;
  box-shadow: 0 2px 8px rgba(79, 172, 254, 0.2);
}

/* Challenge Badge - Orange-Yellow */
.publication-badge.challenge-badge {
  background: transparent;
  color: #ffa726;
  border: 2px solid #ffa726;
}

/* Placeholder Card */
.placeholder-card {
  border: 2px dashed #bdc3c7 !important;
  background: #f8f9fa;
}

.placeholder-img {
  height: 130px;
  background: #ecf0f1;
  border-radius: 8px;
  margin: 10px 10px 0 10px;
}

/* Card Body Styling */
.card-body {
  padding: 0.5rem 1rem 0.75rem 1rem;
}

/* Publication List Item */
.publication-list-item {
  background: white;
  border: 2px solid #e0e0e0;
  border-radius: 8px;
  padding: 20px 24px;
  transition: all 0.3s ease;
  cursor: pointer;
  position: relative;
  min-height: 90px;
  display: flex;
  flex-direction: column;
  justify-content: center;
}

.publication-list-item:hover {
  border-color: #4facfe;
  box-shadow: 0 4px 12px rgba(79, 172, 254, 0.15);
  transform: translateY(-2px);
}

/* Year-specific hover colors */
.year-2025 .publication-list-item:hover {
  border-color: #7a8dd6;
  box-shadow: 0 4px 12px rgba(122, 141, 214, 0.25);
}

.year-2024 .publication-list-item:hover {
  border-color: #f39c12;
  box-shadow: 0 4px 12px rgba(243, 156, 18, 0.25);
}

.year-2023 .publication-list-item:hover {
  border-color: #66b2e4;
  box-shadow: 0 4px 12px rgba(102, 178, 228, 0.25);
}

.year-2022 .publication-list-item:hover {
  border-color: #5fd68f;
  box-shadow: 0 4px 12px rgba(95, 214, 143, 0.25);
}

.year-2021 .publication-list-item:hover {
  border-color: #e788a1;
  box-shadow: 0 4px 12px rgba(231, 136, 161, 0.25);
}

.publication-list-item .publication-badge {
  position: absolute;
  top: 12px;
  right: 16px;
}

.publication-list-item .list-title {
  font-size: 1.1rem;
  font-weight: 600;
  color: #2c3e50;
  margin-bottom: 8px;
  padding-right: 120px;
  line-height: 1.4;
}

.publication-list-item .list-authors {
  font-size: 0.95rem;
  color: #7f8c8d;
  margin-bottom: 0;
  font-weight: 400;
  line-height: 1.4;
}

/* External Links */
.external-links {
  position: absolute;
  right: 16px;
  top: 45px;
  display: flex;
  gap: 10px;
  z-index: 100;
  pointer-events: auto;
}

.external-icon-link {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 32px;
  height: 32px;
  text-decoration: none;
  transition: all 0.3s ease;
  opacity: 1;
  pointer-events: auto;
  cursor: pointer;
}

.external-icon-link:hover {
  transform: scale(1.15);
}

.external-icon-link i {
  font-size: 1.3rem;
}

.external-icon-link.pdf-link {
  color: #dc3545;
}

.external-icon-link.pdf-link:hover {
  color: #c82333;
}

.external-icon-link.website-link {
  color: #28a745;
}

.external-icon-link.website-link:hover {
  color: #218838;
}

.external-icon-link.github-link {
  color: #333;
}

.external-icon-link.github-link:hover {
  color: #000;
}

/* Responsive Design */
@media (max-width: 768px) {
  .category-badge {
    font-size: 1.1rem;
    padding: 10px 20px;
    min-width: 120px;
  }
  
  .placeholder-img {
    height: 120px;
    margin: 10px;
  }
  
  .card-body {
    padding: 1rem;
  }
}

@media (max-width: 576px) {
  .category-header {
    flex-direction: column;
    align-items: flex-start;
  }
  
  .category-badge {
    margin-bottom: 15px;
    margin-right: 0;
  }
  
  .category-line {
    height: 2px;
  }
}

/* Modal Styles */
.publication-modal {
  display: none;
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background: rgba(0, 0, 0, 0.8);
  z-index: 9999;
  justify-content: center;
  align-items: center;
}

.publication-modal.active {
  display: flex;
}

.publication-modal-content {
  background: white;
  border-radius: 15px;
  box-shadow: 0 25px 50px rgba(0, 0, 0, 0.3);
  max-height: 80vh;
  overflow-y: auto;
  position: relative;
  margin: 2rem;
  width: 90vw;
  max-width: 90vw;
}

@media (min-width: 768px) {
  .publication-modal-content {
    width: 75vw;
    max-width: 75vw;
  }
}

@media (min-width: 1200px) {
  .publication-modal-content {
    width: 70vw;
    max-width: 70vw;
  }
}

.publication-modal-header {
  padding: 2rem 2rem 1rem;
  border-bottom: 1px solid #eee;
  position: relative;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
  border-radius: 12px 12px 0 0;
}

/* Year-specific modal header colors */
.modal-2025 .publication-modal-header {
  background: linear-gradient(135deg, #7a8dd6, #8769a8);
}

.modal-2024 .publication-modal-header {
  background: linear-gradient(135deg, #f39c12, #e67e22);
}

.modal-2023 .publication-modal-header {
  background: linear-gradient(135deg, #66b2e4, #4dd5e1);
}

.modal-2022 .publication-modal-header {
  background: linear-gradient(135deg, #5fd68f, #5be1c7);
}

.modal-2021 .publication-modal-header {
  background: linear-gradient(135deg, #e788a1, #eccd6d);
}

.publication-modal-header h2 {
  margin-bottom: 0.5rem;
}

.publication-modal-header .item-intro {
  display: inline-block;
  margin-right: 20px;
}

.publication-modal-body {
  padding: 2rem;
}

.modal-header-links {
  display: inline-flex;
  gap: 15px;
  align-items: center;
  vertical-align: middle;
}

.modal-icon-link {
  display: flex;
  align-items: center;
  justify-content: center;
  width: 38px;
  height: 38px;
  color: #667eea;
  text-decoration: none;
  transition: all 0.3s ease;
  cursor: not-allowed;
  opacity: 0.5;
}

.modal-icon-link:hover {
  color: #764ba2;
  transform: scale(1.1);
  opacity: 0.7;
}

.modal-icon-link i {
  font-size: 1.56rem;
}

/* Active link styles */
.modal-icon-link.active {
  cursor: pointer;
  opacity: 1;
}

.modal-icon-link.active:hover {
  transform: scale(1.15);
  opacity: 1;
}

.modal-icon-link.pdf-link {
  color: #ffffff;
}

.modal-icon-link.pdf-link:hover {
  color: #f0f0f0;
}

.modal-icon-link.website-link {
  color: #ffffff;
}

.modal-icon-link.website-link:hover {
  color: #f0f0f0;
}

.modal-icon-link.github-link {
  color: #ffffff;
}

.modal-icon-link.github-link:hover {
  color: #f0f0f0;
}

.publication-modal-close {
  position: absolute;
  top: 20px;
  right: 25px;
  width: 40px;
  height: 40px;
  cursor: pointer;
  background: none;
  border: none;
  font-size: 30px;
  color: #999;
  display: flex;
  align-items: center;
  justify-content: center;
  z-index: 11;
}

.publication-modal-close:hover {
  color: #333;
}

/* Citation Box Styling */
.citation-box {
  background: #f8f9fa;
  border-left: 4px solid #3498db;
  padding: 15px 20px;
  border-radius: 5px;
  margin: 20px 0;
}

.citation-box p {
  margin: 0;
  line-height: 1.6;
  color: #2c3e50;
}

/* Method Image Section Styling */
.method-image-section {
  margin-bottom: 1.5rem;
}

.method-image-title {
  font-size: 1.1rem;
  font-weight: 600;
  color: #2c3e50;
  margin-bottom: 1rem;
}

.method-image-container {
  width: 100%;
  max-width: 100%;
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
  background: #f8f9fa;
}

.method-image {
  width: 100%;
  height: auto;
  display: block;
  object-fit: contain;
  background: white;
}

/* IPRF Video and Image Styles */
.iprf-scene-row {
  display: flex;
  align-items: stretch;
}

.iprf-image-col, .iprf-video-col {
  display: flex;
  align-items: center;
  justify-content: center;
}

.iprf-video {
  max-width: 100%;
  height: 100%;
  border-radius: 8px;
  background: white;
  object-fit: contain;
}

.iprf-style-image {
  width: 50%;
  aspect-ratio: 1/1;
  object-fit: cover;
  border-radius: 8px;
  background: white;
}

/* Fix transparent background issue */
img, video, .method-image, .publication-thumbnail img {
  background: white !important;
}

</style>

<!-- Publication Modals -->

<!-- SIDL Benchmark Modal -->
<div class="publication-modal modal-2025" id="sidl">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('sidl')">&times;</button>
      <h2 class="text-uppercase">SIDL: A Real-World Dataset for Restoring Smartphone Images with Dirty Lenses</h2>
      <div>
        <p class="item-intro">AAAI Conference on Artificial Intelligence</p>
        <div class="modal-header-links">
          <a href="https://ojs.aaai.org/index.php/AAAI/article/view/32257" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://sidl-benchmark.github.io/" target="_blank" class="modal-icon-link active website-link" title="Website">
            <i class="fas fa-globe"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <!-- Contamination Type Toggles -->
      <div class="text-center mb-4">
        <div class="btn-group btn-group-toggle" data-toggle="buttons" id="contaminantTypes">
          <label class="btn btn-outline-primary active">
            <input type="radio" name="contaminant" id="dust" value="dust" checked> Dust
          </label>
          <label class="btn btn-outline-primary">
            <input type="radio" name="contaminant" id="finger" value="finger"> Fingerprint
          </label>
          <label class="btn btn-outline-primary">
            <input type="radio" name="contaminant" id="scratch" value="scratch"> Scratch
          </label>
          <label class="btn btn-outline-primary">
            <input type="radio" name="contaminant" id="water" value="water"> Water
          </label>
        </div>
      </div>

      <!-- Image Comparison Container -->
      <div class="sidl-comparison-container mb-4" style="position: relative; max-width: 500px; margin: 0 auto;">
        <div class="sidl-image-wrapper" style="position: relative; width: 100%; height: 400px; overflow: hidden; border-radius: 8px; cursor: col-resize;">
          <!-- Target Image (Base) -->
          <img id="sidlTargetImage" class="sidl-image" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover;" src="{{ site.baseurl }}/assets/img/sidl/dust/target/Case073_D.png" alt="Target Image">
          
          <!-- Input Image (Overlay with clip-path) -->
          <div id="sidlInputContainer" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; clip-path: inset(0 50% 0 0);">
            <img id="sidlInputImage" class="sidl-image" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; object-fit: cover;" src="{{ site.baseurl }}/assets/img/sidl/dust/input/Case073_D.png" alt="Input Image">
          </div>
          
          <!-- Vertical Divider Line -->
          <div id="sidlDivider" style="position: absolute; top: 0; left: 50%; width: 2px; height: 100%; background: white; z-index: 10; box-shadow: 0 0 4px rgba(0,0,0,0.3);"></div>
          
          <!-- Slider Handle -->
          <div id="sidlHandle" style="position: absolute; top: 50%; left: 50%; width: 20px; height: 20px; background: white; border-radius: 50%; transform: translate(-50%, -50%); z-index: 11; box-shadow: 0 0 4px rgba(0,0,0,0.3); cursor: col-resize;">
            <div style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); font-size: 10px; color: #666;">⟷</div>
          </div>
        </div>
        
        <!-- Labels -->
        <div class="sidl-labels mt-2">
          <div class="row">
            <div class="col-6 text-center">
              <small class="text-muted"><strong>Input (Contaminated)</strong></small>
            </div>
            <div class="col-6 text-center">
              <small class="text-muted"><strong>Target (Restored)</strong></small>
            </div>
          </div>
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[C21]</strong> Sooyoung Choi*†, Sungyong Park*, and Heewon Kim, "SIDL: A Real-World Dataset for Restoring Smartphone Images with Dirty Lenses," <em>AAAI Conference on Artificial Intelligence (AAAI)</em>, 2025</p>
      </div>
      <p class="mt-3"><small>* Equal contribution<br>† Undergraduate student</small></p>

      <p><strong>Abstract:</strong></p>
      <p>The SIDL (Smartphone Images with Dirty Lenses) dataset addresses a critical gap in image restoration research by providing a comprehensive collection of real-world smartphone images with lens contamination. The dataset contains 300 static scenes with 1,588 image pairs captured in full resolution (4032 × 3024 pixels) 12-bit RAW format. It includes five real-world contaminant types: fingerprints, dust, scratches, water drops, and mixed debris. Scenes span diverse environments and lighting conditions, with images categorized into Easy, Medium, and Hard difficulty levels based on image quality degradation. This dataset's unique contribution is providing diverse real-world images taken under various lighting conditions and environments with paired clean and contaminated images, enabling supervised learning for image restoration techniques.</p>
    </div>
  </div>
</div>

<!-- [C23] BMVC 2025 Modal -->
<div class="publication-modal modal-2025" id="c23">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c23')">&times;</button>
      <h2 class="text-uppercase">Unsupervised Discovery of Transformations between Fine-Grained Classes in Diffusion Models</h2>
      <p class="item-intro">British Machine Vision Conference (BMVC)</p>
    </div>
    <div class="publication-modal-body">
      <!-- Method Image Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Method Overview</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/img/publications/bmvc2025.jpg" alt="BMVC 2025 Method" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[C23]</strong> Youngjae Choi*, Hyunsuh Koh*, Hojae Jeong*, ByungKwan Chae*, Sungyong Park, and Heewon Kim, "Unsupervised Discovery of Transformations between Fine-Grained Classes in Diffusion Models," <em>Proc. British Machine Vision Conference (BMVC)</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>
    </div>
  </div>
</div>

<!-- [C24] APP3DV 2025 Modal -->
<div class="publication-modal modal-2025" id="c24">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c24')">&times;</button>
      <h2 class="text-uppercase">Intrinsic-Guided Photorealistic Style Transfer for Radiance Fields</h2>
      <div>
        <p class="item-intro">International Workshop on Application-driven Point Cloud Processing and 3D Vision (APP3DV)</p>
        <div class="modal-header-links">
          <a href="https://drive.google.com/file/d/10vp2SWByAJGr-Ccrw68c2hTepsMr5di9/view" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://oshmos.github.io/IPRF/" target="_blank" class="modal-icon-link active website-link" title="Website">
            <i class="fas fa-globe"></i>
          </a>
          <a href="https://github.com/OSHMOS/IPRF" target="_blank" class="modal-icon-link active github-link" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <!-- Video Results Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title" style="font-size: 120%;">Results</h5>

        <!-- Column Headers -->
        <div class="row mb-3">
          <div class="col-md-3 text-center">
            <strong>Style Image</strong>
          </div>
          <div class="col-md-9">
            <div style="width: 80%; margin: 0 auto; display: grid; grid-template-columns: 1fr 1fr 1fr; text-align: center;">
              <strong>Plenoxels</strong>
              <strong>FPRF</strong>
              <strong>IPRF (Ours)</strong>
            </div>
          </div>
        </div>

        <!-- Flower Scene -->
        <div class="row mb-4 iprf-scene-row">
          <div class="col-md-3 iprf-image-col">
            <img src="{{ site.baseurl }}/assets/img/publications/iprf/14.jpg" alt="Style Image" class="iprf-style-image">
          </div>
          <div class="col-md-9 iprf-video-col">
            <video controls autoplay loop muted class="iprf-video">
              <source src="{{ site.baseurl }}/assets/img/publications/iprf/flower_14.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <!-- Horns Scene -->
        <div class="row mb-4 iprf-scene-row">
          <div class="col-md-3 iprf-image-col">
            <img src="{{ site.baseurl }}/assets/img/publications/iprf/139.jpg" alt="Style Image" class="iprf-style-image">
          </div>
          <div class="col-md-9 iprf-video-col">
            <video controls autoplay loop muted class="iprf-video">
              <source src="{{ site.baseurl }}/assets/img/publications/iprf/horns_139.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <!-- Room Scene -->
        <div class="row mb-4 iprf-scene-row">
          <div class="col-md-3 iprf-image-col">
            <img src="{{ site.baseurl }}/assets/img/publications/iprf/122.jpg" alt="Style Image" class="iprf-style-image">
          </div>
          <div class="col-md-9 iprf-video-col">
            <video controls autoplay loop muted class="iprf-video">
              <source src="{{ site.baseurl }}/assets/img/publications/iprf/room_122.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>

      <!-- Pipeline Image Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title" style="font-size: 120%;">Pipeline</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/assets/img/publications/iprf/pipeline.png" alt="IPRF Pipeline" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[C24]</strong> Hyunsuh Koh*, Seunghyun Oh*, Jungyun Jang*, and Heewon Kim, "Intrinsic-Guided Photorealistic Style Transfer for Radiance Fields," <em>Proc. International Workshop on Application-driven Point Cloud Processing and 3D Vision (APP3DV, ACM MM Workshop)</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>Photorealistic style transfer in neural radiance fields (NeRF) aims to modify the color characteristics of a 3D scene without altering its underlying geometry. Although recent approaches have achieved promising results, they often suffer from limited style diversity, focusing primarily on global color shifts. In contrast, artistic style transfer methods offer richer stylization but usually distort scene geometry, thereby reducing realism.</p>

      <p>In this work, we present Intrinsic-guided Photorealistic Style Transfer (IPRF), a novel framework that leverages intrinsic image decomposition to decouple a scene into albedo and shading components. By introducing tailored loss functions in both domains, IPRF aligns the texture and color of the content scene to those of a style image while faithfully preserving geometric structure and lighting.</p>

      <p>Furthermore, we propose Tuning-assisted Style Interpolation (TSI), a real-time technique for exploring the trade-off between photorealism and artistic expression through a weighted combination of albedo-oriented and shading-oriented radiance fields. Experimental results demonstrate that IPRF achieves a superior balance between naturalism and artistic expression compared to state-of-the-art methods, offering a versatile solution for 3D content creation in various fields, including digital art, virtual reality, and game design.</p>
    </div>
  </div>
</div>

<!-- [J12] PLOS ONE 2025 Modal -->
<div class="publication-modal modal-2025" id="j12">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j12')">&times;</button>
      <h2 class="text-uppercase">DeepGAM: An Interpretable Deep Neural Network Using Generalized Additive Model for Depression Diagnosis</h2>
      <p class="item-intro">PLOS ONE</p>
    </div>
    <div class="publication-modal-body">
      <!-- Network Architecture Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title" style="font-size: 120%;">Network Architecture</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/assets/img/publications/deepgam/architecture.png" alt="DeepGAM Architecture" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[J12]</strong> Chiyoung Lee*, Yeri Kim*†, Seoyoung Kim*†, Mary Whooley, and Heewon Kim, "DeepGAM: An Interpretable Deep Neural Network Using Generalized Additive Model for Depression Diagnosis," <em>PLOS ONE</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>* Equal contribution<br>† Undergraduate student</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>Deep neural networks have achieved significant performance breakthroughs across a range of tasks. For diagnosing depression, there has been increasing attention on estimating depression status from personal medical data. However, the neural networks often act as black boxes, making it difficult to discern the individual effects of each input component. To alleviate this problem, we proposed a deep-learning-based generalized additive model called DeepGAM to improve the interpretability of depression diagnosis.</p>

      <p>We utilized the baseline cross-sectional data from the Heart and Soul Study to achieve our study's aim. DeepGAM incorporates additive functions based on a neural network that learns to discern the positive and negative impacts of the values of individual components. The network architecture and the objective function are designed to constrain and regularize the output values for interpretability. Moreover, we used a direct-through estimator (STE) to select important features using gradient descent. The STE enables machine learning models to maintain their performance using a few features and interpretable function visualizations.</p>

      <p>DeepGAM achieved the highest AUC (0.600) and F1-score (0.387), outperforming neural networks and IGANN. The five features selected via STE performed comparably to 99 features and surpassed traditional methods such as Lasso and Boruta. Additionally, analyses highlighted DeepGAM's interpretability and performance on public datasets. In conclusion, DeepGAM with STE demonstrated accurate and interpretable performance in predicting depression compared to existing machine learning methods.</p>
    </div>
  </div>
</div>

<!-- ARNOLD Challenge Modal -->
<div class="publication-modal modal-2025" id="arnold">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('arnold')">&times;</button>
      <h2 class="text-uppercase">1st Place Winner at CVPR 2025 Embodied AI Workshop</h2>
      <p class="item-intro">ARNOLD Challenge</p>
    </div>
    <div class="publication-modal-body">
      <p><strong>Team:</strong> Reality Lab Team</p>
      <p><strong>Event:</strong> CVPR 2025 Embodied AI Workshop</p>
      <p><strong>Achievement:</strong> 1st Place 🥇</p>
      <p><strong>Description:</strong> Our team achieved first place in the ARNOLD Challenge, demonstrating excellence in embodied AI and robotics research.</p>
      <p><strong>Challenge Areas:</strong></p>
      <ul>
        <li>Embodied AI</li>
        <li>Robotics</li>
        <li>Navigation</li>
        <li>Human-Robot Interaction</li>
        <li>Computer Vision</li>
      </ul>
      <p><strong>Key Achievements:</strong></p>
      <ul>
        <li>First place among international teams</li>
        <li>Outstanding performance in navigation tasks</li>
        <li>Advanced AI-human interaction capabilities</li>
        <li>Real-world robotic system deployment</li>
      </ul>
    </div>
  </div>
</div>

<!-- [J11] JEET 2025 Modal -->
<div class="publication-modal modal-2025" id="j11">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j11')">&times;</button>
      <h2 class="text-uppercase">Dog Cough Sound Classification Using Neural Networks for Diagnosing Bronchial Diseases</h2>
      <p class="item-intro">Journal of Electrical Engineering & Technology (JEET)</p>
    </div>
    <div class="publication-modal-body">
      <!-- Method Image Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Method Overview</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/img/publications/jeet2025.jpg" alt="JEET 2025 Method" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[J11]</strong> Do-Ye Kwon*†, Yeon-Ju Oh*†, and Heewon Kim, "Dog Cough Sound Classification Using Neural Networks for Diagnosing Bronchial Diseases," <em>Journal of Electrical Engineering & Technology (JEET)</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>* Equal contribution<br>† Undergraduate student</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>We propose a diagnostic system for identifying bronchial diseases by analyzing dog cough sounds. We collected a dataset consisting of 124 healthy dog cough sounds obtained from open sources and 94 dog cough sounds with bronchial diseases obtained from YouTube, and performed a total of 218 recordings. These cough sounds were segmented into 423 separate cough datasets to improve the details and accuracy of their analysis. Additionally, data augmentation techniques such as noise addition, pitch shifting, time stretching, and volume scaling were applied, increasing the dataset size by 7 times. This resulted in 1,526 training and testing samples for multiple coughs and 2,961 samples for single coughs.</p>

      <p>The disease prediction system leverages three different neural network models, multilayer perceptron (MLP), convolutional neural network (CNN), and recurrent neural network (RNN), to evaluate their effectiveness in detecting bronchial diseases. In our experiments, we found that the single cough dataset outperformed the multiple cough dataset, with the CNN achieving the highest accuracy, precision, AUC, and F1 scores compared to the RNN and MLP. The study highlights the potential of machine learning in improving diagnostic accuracy for veterinary medicine, suggesting that integrating different models could enhance diagnostic tools, thereby contributing to better health outcomes for dogs.</p>
    </div>
  </div>
</div>

<!-- [J10] ICT Express 2025 Modal -->
<div class="publication-modal modal-2025" id="j10">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j10')">&times;</button>
      <h2 class="text-uppercase">Accurate Baseball Player Pose Refinement Using Motion Prior Guidance</h2>
      <p class="item-intro">ICT Express</p>
    </div>
    <div class="publication-modal-body">
      <!-- Method Image Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Method Overview</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/img/publications/ictexpress2025.jpg" alt="ICT Express 2025 Method" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[J10]</strong> Seunghyun Oh† and Heewon Kim, "Accurate Baseball Player Pose Refinement Using Motion Prior Guidance," <em>ICT Express</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>† Undergraduate student</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>Human pose estimation (HPE) is challenging due to the need to accurately capture rapid and occluded body movements, often resulting in uncertain predictions. In the context of fast sports actions like baseball swings, existing HPE methods insufficiently leverage domain-specific prior knowledge about these movements. To address this gap, we propose the Baseball Player Pose Corrector (BPPC), an optimization framework that utilizes high-quality 3D standard motion data to refine 2D keypoints in baseball swing videos. BPPC operates in two stages: first, it aligns the 3D standard motion to test swing videos through action recognition, offset learning, and 3D-to-2D projection. Next, it applies movement-aware optimization to refine the keypoints, ensuring robustness to variations in swing patterns.</p>

      <p>Notably, BPPC does not rely on additional datasets; it only requires manually annotated 3D standard motion data for baseball swings. Experimental results demonstrate that BPPC improves keypoint estimation accuracy by up to 2.4% on a baseball swing dataset, particularly enhancing keypoints with confidence scores below 0.5. Qualitative analysis further highlights BPPC's ability to correct rapidly moving joints, such as elbows and wrists.</p>
    </div>
  </div>
</div>

<!-- [C22] CVPR 2025 Modal - DynScene -->
<div class="publication-modal modal-2025" id="c22">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c22')">&times;</button>
      <h2 class="text-uppercase">DynScene: Scalable Generation of Dynamic Robotic Manipulation Scenes for Embodied AI</h2>
      <div>
        <p class="item-intro">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Choi_Motion-Aware_Dynamic_Architecture_for_Efficient_Frame_Interpolation_ICCV_2021_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://cvpr.thecvf.com/virtual/2025/poster/33953" target="_blank" class="modal-icon-link website-link" title="Website">
            <i class="fas fa-globe"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <!-- Method Image Section -->
      <div class="method-image-section mb-4">
        <h5 class="method-image-title">Method Overview</h5>
        <div class="method-image-container">
          <img src="{{ site.baseurl }}/img/publications/cvpr2025.jpg" alt="CVPR 2025 Method" class="method-image">
        </div>
      </div>

      <div class="citation-box">
        <p><strong>[C22]</strong> Sangmin Lee*, Sungyong Park*, and Heewon Kim, "DynScene: Scalable Generation of Dynamic Robotic Manipulation Scenes for Embodied AI," <em>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>Creating robotic manipulation datasets is traditionally labor-intensive and expansive, requiring extensive manual effort. To alleviate this problem, we introduce PhaseScene, which generates realistic and diverse dynamic scenes (or robotic manipulation data) from text instructions for Embodied AI. PhaseScene employs a phase-specific data representation by dividing dynamic scenes into static environments and robot movements. Each phase utilizes a diffusion-based method to generate phase-specific data, incorporating data refinement and augmentation techniques.</p>

      <p>Our experiments demonstrate that PhaseScene outperforms human creation by about 20 times faster speed, 1.84 times accuracy, and 28% higher action diversity based on standard metrics. Additionally, the generated scenes enable accurate agent training with an average success rate improvement of 7.96% for PerAct and 11.23% for PerAct-PSA.</p>
    </div>
  </div>
</div>

<!-- [C20&J9] Brain Stimulation 2025 Modal -->
<div class="publication-modal modal-2025" id="c20j9">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c20j9')">&times;</button>
      <h2 class="text-uppercase">Predictors of the treatment effects of transcranial direct current stimulation on knee osteoarthritis pain</h2>
      <div>
        <p class="item-intro">Brain Stimulation</p>
        <div class="modal-header-links">
          <a href="https://www.brainstimjrnl.com/article/S1935-861X%2824%2900905-7/fulltext" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="#" class="modal-icon-link" onclick="return false;" title="Website">
            <i class="fas fa-globe"></i>
          </a>
          <a href="#" class="modal-icon-link" onclick="return false;" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C20&J9]</strong> Chiyoung Lee, Heewon Kim, Yeri Kim†, Seoyoung Kim†, et al., "Predicting tDCS Treatment Effects Using Machine Learning," <em>Proc. International Brain Stimulation Conference & Brain Stimulation Journal</em>, 2025 (accepted)</p>
      </div>
      <p class="mt-3"><small>† Undergraduate student</small></p>

      <p style="font-size: 120%;"><strong>Abstract:</strong></p>
      <p>The study's objective was to develop a clinical prediction rule that predicts a clinically significant analgesic effect on chronic knee osteoarthritis pain after transcranial direct current stimulation treatment. This is a secondary analysis from a double-blind randomized controlled trial. Data from 51 individuals with chronic knee osteoarthritis pain and an impaired descending pain inhibitory system were used. The intervention comprised a 15-session protocol of anodal primary motor cortex transcranial direct current stimulation. Treatment success was defined by the Western Ontario and McMaster Universities' Osteoarthritis Index pain subscale. Accuracy statistics were calculated for each potential predictor and for the final model.</p>

      <p>The final logistic regression model was statistically significant (p < 0.01) and comprised five physical and psychosocial predictor variables that together yielded a positive likelihood ratio of 14.40 (95% CI: 3.66–56.69) and an 85% (95%CI: 60–96%) post-test probability of success. This is the first clinical prediction rule proposed for transcranial direct current stimulation in patients with chronic pain. The model underscores the importance of both physical and psychosocial factors as predictors of the analgesic response to transcranial direct current stimulation treatment. Validation of the proposed clinical prediction rule should be performed in other datasets.</p>
    </div>
  </div>
</div>

<!-- 2024 Modals -->

<!-- [C17&J8] Brain Stimulation 2024 Modal -->
<div class="publication-modal" id="c17j8">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c17j8')">&times;</button>
      <h2 class="text-uppercase">[C17&J8] Brain Stimulation 2024</h2>
      <p class="item-intro text-muted">Predicting tDCS Treatment Effects in Depression Using Deep Learning</p>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C17&J8]</strong> Chiyoung Lee, Heewon Kim, Seoyoung Kim†, Yeri Kim†, et al., "Predicting tDCS Treatment Effects in Depression Using Deep Learning," <em>Proc. International Brain Stimulation Conference & Brain Stimulation Journal</em>, 2024</p>
      </div>
      <p class="mt-3"><small>† Undergraduate student</small></p>
    </div>
  </div>
</div>

<!-- [J7] MDPI IJMS 2024 Modal -->
<div class="publication-modal" id="j7">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j7')">&times;</button>
      <h2 class="text-uppercase">[J7] MDPI IJMS 2024</h2>
      <div>
        <p class="item-intro text-muted">Machine Learning-Based Etiologic Subtyping of Ischemic Stroke</p>
        <div class="modal-header-links">
          <a href="https://www.mdpi.com/1422-0067/25/12/6761" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J7]</strong> Seoyoung Kim, Chiyoung Lee, Jiyeon Park, Hyo Suk Nam, and Heewon Kim, "Machine Learning-Based Etiologic Subtyping of Ischemic Stroke Using Circulating Exosomal microRNAs," <em>International Journal of Molecular Sciences (IJMS)</em>, vol. 25, no. 12, 2024</p>
      </div>
    </div>
  </div>
</div>

<!-- [C16] AAAI 2024 Modal -->
<div class="publication-modal" id="c16">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c16')">&times;</button>
      <h2 class="text-uppercase">[C16] AAAI 2024</h2>
      <p class="item-intro text-muted">Language-Guided Robotic Manipulation</p>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C16]</strong> Sangmin Lee*, Sungyong Park*, and Heewon Kim, "Language-Guided Robotic Manipulation with Vision-Language Models," <em>Proc. AAAI Conference on Artificial Intelligence (AAAI)</em>, 2024</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>
    </div>
  </div>
</div>

<!-- [J6] ICT Express 2024 Modal -->
<div class="publication-modal" id="j6">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j6')">&times;</button>
      <h2 class="text-uppercase">[J6] ICT Express 2024</h2>
      <p class="item-intro text-muted">Baseball Player Pose Estimation</p>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J6]</strong> Seunghyun Oh† and Heewon Kim, "Baseball Player Pose Estimation Using Deep Learning," <em>ICT Express</em>, vol. 10, no. 1, 2024</p>
      </div>
      <p class="mt-3"><small>† Undergraduate student</small></p>
    </div>
  </div>
</div>

<!-- 2023 Modals -->

<!-- [C15] ICLR 2023 Modal -->
<div class="publication-modal" id="c15">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c15')">&times;</button>
      <h2 class="text-uppercase">[C15] ICLR 2023</h2>
      <div>
        <p class="item-intro text-muted">NERDS: A General Framework to Train Camera Denoisers</p>
        <div class="modal-header-links">
          <a href="https://openreview.net/forum?id=NO0ThzteQdI" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C15]</strong> Heewon Kim and Kyoung Mu Lee, "NERDS: A General Framework to Train Camera Denoisers from Raw-RGB Noisy Image Pairs," <em>The Eleventh International Conference on Learning Representations (ICLR)</em>, 2023</p>
      </div>
    </div>
  </div>
</div>

<!-- [C14] WACV 2022 Modal -->
<div class="publication-modal" id="c14">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c14')">&times;</button>
      <h2 class="text-uppercase">[C14] WACV 2022</h2>
      <div>
        <p class="item-intro text-muted">DAQ: Channel-Wise Distribution-Aware Quantization</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Hong_DAQ_Channel-Wise_Distribution-Aware_Quantization_for_Deep_Image_Super-Resolution_Networks_WACV_2022_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C14]</strong> Cheeun Hong, Heewon Kim, Sungyong Baik, Junghun Oh, and Kyoung Mu Lee, "DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks," <em>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022</p>
      </div>
    </div>
  </div>
</div>

<!-- [C13] WACV 2022 Modal -->
<div class="publication-modal" id="c13">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c13')">&times;</button>
      <h2 class="text-uppercase">[C13] WACV 2022</h2>
      <div>
        <p class="item-intro text-muted">Batch Normalization Tells You Which Filter is Important</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content/WACV2022/papers/Oh_Batch_Normalization_Tells_You_Which_Filter_Is_Important_WACV_2022_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C13]</strong> Junghun Oh, Saehoon Kim, Namhoon Lee, and Heewon Kim, "Batch Normalization Tells You Which Filter is Important," <em>Proc. IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2022</p>
      </div>
    </div>
  </div>
</div>

<!-- [J5] PLOS ONE 2022 Modal -->
<div class="publication-modal" id="j5">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j5')">&times;</button>
      <h2 class="text-uppercase">[J5] PLOS ONE 2022</h2>
      <div>
        <p class="item-intro text-muted">Machine learning-based predictive modeling of depression in hypertensive populations</p>
        <div class="modal-header-links">
          <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0272330" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J5]</strong> Seoyoung Kim, Yeri Kim, Chiyoung Lee, Juyoung Park, Hyochol Ahn, and Heewon Kim, "Machine learning-based predictive modeling of depression in hypertensive populations," <em>PLOS ONE</em>, vol. 17, no. 7, 2022</p>
      </div>
    </div>
  </div>
</div>

<!-- 2022 Modals -->

<!-- [C12] ECCV 2022 Modal -->
<div class="publication-modal" id="c12">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c12')">&times;</button>
      <h2 class="text-uppercase">[C12] ECCV 2022</h2>
      <div>
        <p class="item-intro text-muted">CADyQ: Content-Aware Dynamic Quantization</p>
        <div class="modal-header-links">
          <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136670360.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://github.com/Cheeun/CADyQ" target="_blank" class="modal-icon-link active github-link" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C12]</strong> Cheeun Hong*, Heewon Kim*, Sungyong Baik, Junghun Oh, and Kyoung Mu Lee, "CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution," <em>Proc. European Conference on Computer Vision (ECCV)</em>, 2022</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>
    </div>
  </div>
</div>

<!-- [J4] IEEE TIP 2024 Modal -->
<div class="publication-modal" id="j4">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j4')">&times;</button>
      <h2 class="text-uppercase">[J4] IEEE TIP 2024</h2>
      <div>
        <p class="item-intro text-muted">Learning Controllable ISP for Image Enhancement</p>
        <div class="modal-header-links">
          <a href="https://ieeexplore.ieee.org/document/10225702" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J4]</strong> Heewon Kim and Kyoung Mu Lee, "Learning Controllable ISP for Image Enhancement," <em>IEEE Transactions on Image Processing</em>, vol. 33, 2024</p>
      </div>
    </div>
  </div>
</div>

<!-- [C11] CVPR 2022 Modal -->
<div class="publication-modal" id="c11">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c11')">&times;</button>
      <h2 class="text-uppercase">[C11] CVPR 2022</h2>
      <div>
        <p class="item-intro text-muted">Attentive Fine-Grained Structured Sparsity for Image Restoration</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Oh_Attentive_Fine-Grained_Structured_Sparsity_for_Image_Restoration_CVPR_2022_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C11]</strong> Junghun Oh, Heewon Kim, Seungjun Nah, Cheeun Hong, Jonghyun Choi, and Kyoung Mu Lee, "Attentive Fine-Grained Structured Sparsity for Image Restoration," <em>Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2022</p>
      </div>
    </div>
  </div>
</div>

<!-- 2021 and Earlier Modals -->

<!-- [C10] ICCV 2021 Modal -->
<div class="publication-modal" id="c10">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c10')">&times;</button>
      <h2 class="text-uppercase">[C10] ICCV 2021</h2>
      <div>
        <p class="item-intro text-muted">MeTAL: Meta-Learning with Task-Adaptive Loss Function</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Baik_Meta-Learning_With_Task-Adaptive_Loss_Function_for_Few-Shot_Learning_ICCV_2021_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://github.com/baiksung/MeTAL" target="_blank" class="modal-icon-link active github-link" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C10]</strong> Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho, Jaesik Min, and Kyoung Mu Lee, "Meta-Learning with Task-Adaptive Loss Function for Few-Shot Learning," <em>Proc. IEEE International Conference on Computer Vision (ICCV)</em>, 2021 (Oral)</p>
      </div>
    </div>
  </div>
</div>

<!-- [J3] IEEE TETCI 2020 Modal -->
<div class="publication-modal" id="j3">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j3')">&times;</button>
      <h2 class="text-uppercase">[J3] IEEE TETCI 2020</h2>
      <div>
        <p class="item-intro text-muted">Learning to Learn Task-Adaptive Hyperparameters for Few-Shot Learning</p>
        <div class="modal-header-links">
          <a href="https://ieeexplore.ieee.org/document/10080995" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J3]</strong> Sungyong Baik, Janghoon Choi, Heewon Kim, Dohee Cho, Jaesik Min, and Kyoung Mu Lee, "Learning to Learn Task-Adaptive Hyperparameters for Few-Shot Learning," <em>IEEE Transactions on Emerging Topics in Computational Intelligence</em>, 2020</p>
      </div>
    </div>
  </div>
</div>

<!-- [C9] NeurIPS 2020 Modal -->
<div class="publication-modal" id="c9">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c9')">&times;</button>
      <h2 class="text-uppercase">[C9] NeurIPS 2020</h2>
      <div>
        <p class="item-intro text-muted">ALFA: Meta-Learning with Adaptive Hyperparameters</p>
        <div class="modal-header-links">
          <a href="https://papers.nips.cc/paper/2020/file/ee89223a2b625b5152132ed77abbcc79-Paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://github.com/baiksung/ALFA" target="_blank" class="modal-icon-link active github-link" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C9]</strong> Sungyong Baik, Myungsub Choi, Janghoon Choi, Heewon Kim, and Kyoung Mu Lee, "Meta-Learning with Adaptive Hyperparameters," <em>Proc. Conference on Neural Information Processing Systems (NeurIPS)</em>, 2020</p>
      </div>
    </div>
  </div>
</div>

<!-- [J2] JVCIR 2022 Modal -->
<div class="publication-modal" id="j2">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j2')">&times;</button>
      <h2 class="text-uppercase">[J2] JVCIR 2022</h2>
      <div>
        <p class="item-intro text-muted">Fine-grained neural architecture search for image super-resolution</p>
        <div class="modal-header-links">
          <a href="https://www.sciencedirect.com/science/article/abs/pii/S1047320322001742" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J2]</strong> Heewon Kim and Kyoung Mu Lee, "Fine-grained neural architecture search for image super-resolution," <em>Journal of Visual Communication and Image Representation (JVCIR)</em>, vol. 88, 2022</p>
      </div>
    </div>
  </div>
</div>

<!-- [C8] CVPR 2017 Modal -->
<div class="publication-modal" id="c8">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('c8')">&times;</button>
      <h2 class="text-uppercase">[C8] CVPR 2017</h2>
      <div>
        <p class="item-intro text-muted">EDSR: Enhanced Deep Residual Networks for Single Image Super-Resolution</p>
        <div class="modal-header-links">
          <a href="https://openaccess.thecvf.com/content_cvpr_2017_workshops/w12/papers/Lim_Enhanced_Deep_Residual_CVPR_2017_paper.pdf" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
          <a href="https://github.com/sanghyun-son/EDSR-PyTorch" target="_blank" class="modal-icon-link active github-link" title="GitHub">
            <i class="fab fa-github"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[C8]</strong> Bee Lim*, Sanghyun Son*, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee, "Enhanced Deep Residual Networks for Single Image Super-Resolution," <em>Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em>, 2017</p>
      </div>
      <p class="mt-3"><small>* Equal contribution</small></p>
    </div>
  </div>
</div>

<!-- [J1] AAAI 2020 Modal -->
<div class="publication-modal" id="j1">
  <div class="publication-modal-content">
    <div class="publication-modal-header">
      <button class="publication-modal-close" onclick="closePublicationModal('j1')">&times;</button>
      <h2 class="text-uppercase">[J1] AAAI 2020</h2>
      <div>
        <p class="item-intro text-muted">Channel Attention Is All You Need for Video Frame Interpolation</p>
        <div class="modal-header-links">
          <a href="https://www.dropbox.com/scl/fi/8km8l254msd8rm8cjfkct/AAAI-ChoiM.4773.pdf?rlkey=rws6k9ctw5prhis162zk30xts&dl=0" target="_blank" class="modal-icon-link active pdf-link" title="PDF">
            <i class="fas fa-file-pdf"></i>
          </a>
        </div>
      </div>
    </div>
    <div class="publication-modal-body">
      <div class="citation-box">
        <p><strong>[J1]</strong> Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and Kyoung Mu Lee, "Channel Attention Is All You Need for Video Frame Interpolation," <em>Proc. AAAI Conference on Artificial Intelligence (AAAI)</em>, 2020</p>
      </div>
    </div>
  </div>
</div>

<script>
function openPublicationModal(modalId) {
  const modal = document.getElementById(modalId);
  if (modal) {
    modal.classList.add("active");
    document.body.style.overflow = "hidden";
  }
}

function closePublicationModal(modalId) {
  const modal = document.getElementById(modalId);
  if (modal) {
    modal.classList.remove("active");
    document.body.style.overflow = "";
  }
}

// Close modal when clicking outside content
document.addEventListener("click", function(event) {
  if (event.target.classList.contains("publication-modal")) {
    const modalId = event.target.id;
    closePublicationModal(modalId);
  }
});

// Close modal with Escape key
document.addEventListener("keydown", function(event) {
  if (event.key === "Escape") {
    const activeModal = document.querySelector(".publication-modal.active");
    if (activeModal) {
      closePublicationModal(activeModal.id);
    }
  }
});

// Open modal from URL hash (for home page redirects)
document.addEventListener("DOMContentLoaded", function() {
  const hash = window.location.hash;
  if (hash && hash.endsWith("-open")) {
    const modalId = hash.substring(1).replace("-open", "");
    openPublicationModal(modalId);
    // Clean up URL
    history.replaceState(null, null, window.location.pathname);
  }

  // Add year classes to modals based on their title
  const modals = document.querySelectorAll(".publication-modal");
  modals.forEach(modal => {
    const title = modal.querySelector("h2");
    if (title) {
      const titleText = title.textContent;
      if (titleText.includes("2025")) {
        modal.classList.add("modal-2025");
      } else if (titleText.includes("2024")) {
        modal.classList.add("modal-2024");
      } else if (titleText.includes("2023")) {
        modal.classList.add("modal-2023");
      } else if (titleText.includes("2022")) {
        modal.classList.add("modal-2022");
      } else if (titleText.includes("2021") || titleText.includes("2020") || titleText.includes("2017")) {
        modal.classList.add("modal-2021");
      }
    }
  });
});
</script>